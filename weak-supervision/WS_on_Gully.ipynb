{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afef4df4-daff-47ec-82ea-15c732f6ff96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import labeling_function\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import imageio.v2 as imageio\n",
    "from LF_library import *\n",
    "from LF_deep_utils import *\n",
    "from LF_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff75437f-7cd3-4a10-82e0-6d626510938b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# # File paths\n",
    "# file1_path = './vlm_responses/finla_results_llama3.2-vision_90b.json'\n",
    "# file2_path = './vlm_responses/test_data/test_results_llama3.2-vision_90b.json'\n",
    "# output_path = './vlm_responses/llama_merged.json'\n",
    "\n",
    "# # Load the JSON files\n",
    "# with open(file1_path, 'r') as file1:\n",
    "#     data1 = json.load(file1)\n",
    "\n",
    "# with open(file2_path, 'r') as file2:\n",
    "#     data2 = json.load(file2)\n",
    "\n",
    "# # Modify keys in the second JSON\n",
    "# modified_data2 = {f\"Dev-{key}\": value for key, value in data2.items()}\n",
    "\n",
    "# # Merge the dictionaries\n",
    "# merged_data = {**data1, **modified_data2}\n",
    "\n",
    "# # Save the merged dictionary into a new JSON file\n",
    "# with open(output_path, 'w') as output_file:\n",
    "#     json.dump(merged_data, output_file, indent=4)\n",
    "\n",
    "# print(f\"Merged JSON saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa6f2ac-cdb5-45a9-81c7-fd12058c61cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e883b66-60a8-48fc-a0f9-5fbe0f6e8f05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9dd70f09-fc21-423d-9784-47651ec6e3cd",
   "metadata": {},
   "source": [
    "# Defining the Labelers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71386fb0-f98d-4e26-9e70-30b5b34cd469",
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIVE = 1\n",
    "NEGATIVE = 0\n",
    "ABSTAIN = -1\n",
    "\n",
    "@labeling_function()\n",
    "def Qwen(image_name):\n",
    "    file_path = './vlm_responses/qwen_merged.json'\n",
    "    # llava_7b_results = \n",
    "    # path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # for key in data:\n",
    "    #     if data[key] == '4':\n",
    "    #         data[key] = 1\n",
    "    #     else:\n",
    "    #         data[key] = 0\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1\n",
    "\n",
    "@labeling_function()\n",
    "def Llama3_vision(image_name):\n",
    "    root_path = './vlm_responses/llama_merged.json'\n",
    "    # llava_7b_results = 'finla_results_llama3.2-vision_90b.json'\n",
    "    # path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(root_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    for key in data:\n",
    "        if data[key] == [\"4\"]:\n",
    "            data[key] = 1\n",
    "        else:\n",
    "            data[key] = 0\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1\n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def deeplearning(image_name):\n",
    "    root_path = './vlm_responses/deeplearning_merged.json'\n",
    "    # llava_7b_results = './train_deep_learning_labels.json'\n",
    "    # path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(root_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1\n",
    "\n",
    "@labeling_function()\n",
    "def line_detection(image_name):\n",
    "    root_path = './vlm_responses/line_detection_merged.json'\n",
    "    # llava_7b_results = './train_line_detection_labels.json'\n",
    "    # path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(root_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1\n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def superpixel_detection(image_name):\n",
    "    root_path = './vlm_responses/superpixel_merged.json'\n",
    "    # llava_7b_results = './train_super_pixel_labels.json'\n",
    "    # path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(root_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    for key in data:\n",
    "        if data[key] == -1:\n",
    "            data[key] = 1\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e65909b-4186-42eb-9142-d55cfcc81b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "superpixel_detection(\"1005\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a054ab41-d025-4800-97d0-a2661422f8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "['4']\n"
     ]
    }
   ],
   "source": [
    "train_data_json_path = './vlm_responses/finla_results_llama3.2-vision_90b.json'\n",
    "# dev_data_json_path = '../prompting_framework/prompting_results/hateful/simplified_dev.json'\n",
    "\n",
    "with open(train_data_json_path, 'r') as file:\n",
    "    train_data = json.load(file)\n",
    "\n",
    "# Extract and pad image names, ensuring they are 5 digits long before the '.png'\n",
    "train_image_names = []\n",
    "for key in train_data:\n",
    "    print(key)\n",
    "    print(train_data[key])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff94abf-a386-4f28-a46e-835e955e8c47",
   "metadata": {},
   "source": [
    "# Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e3bc313-ce3f-4875-b92f-8ecd100992de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8518 images in the Train set.\n",
      "There are 311 images in the dev set.\n",
      "There are 311 labels in the dev set.\n"
     ]
    }
   ],
   "source": [
    "train_data_json_path = './vlm_responses/finla_results_llama3.2-vision_90b.json'\n",
    "dev_data_json_path = './vlm_responses/test_data/Testing_Set.json'\n",
    "\n",
    "with open(train_data_json_path, 'r') as file:\n",
    "    train_data = json.load(file)\n",
    "\n",
    "# Extract and pad image names, ensuring they are 5 digits long before the '.png'\n",
    "train_image_names = []\n",
    "for entry in train_data:\n",
    "    # img_name, ext = entry['img'].split('.')\n",
    "    # padded_img_name = img_name.zfill(5)  # Pad the image name to 5 digits\n",
    "    train_image_names.append(str(entry))\n",
    "\n",
    "with open(dev_data_json_path, 'r') as file:\n",
    "    dev_data = json.load(file)\n",
    "    \n",
    "dev_image_names = []\n",
    "Y_dev = []\n",
    "for entry in dev_data:\n",
    "    Y_dev.append(int(int(dev_data[entry]['label'])/4))\n",
    "    # img_name, ext = entry['img'].split('.')\n",
    "    # padded_img_name = img_name.zfill(5)  # Pad the image name to 5 digits\n",
    "    dev_image_names.append(f\"Dev-{entry}\")\n",
    "\n",
    "print(f\"There are {len(train_image_names)} images in the Train set.\")\n",
    "print(f\"There are {len(dev_image_names)} images in the dev set.\")\n",
    "print(f\"There are {len(Y_dev)} labels in the dev set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f03af0-f77d-437e-8fce-b78ab9ca7463",
   "metadata": {},
   "outputs": [],
   "source": [
    "Qwen(dev_image_names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff78e3e-467b-4fae-a403-4dc534228bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_image_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d5cecc-7c43-4b50-b2a8-6b31b253dc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b5df0f-fdba-4f11-9d99-e7cfaf24c44f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Applying the LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3f7ca650-4742-43d6-bb6f-c853dd176269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import LFApplier\n",
    "\n",
    "list_of_all_the_models = ['Qwen', 'Llama3_vision', 'deeplearning', 'line_detection', 'superpixel_detection'\n",
    "       ]\n",
    "\n",
    "lfs = [Qwen, Llama3_vision, deeplearning, line_detection, superpixel_detection\n",
    "       ]\n",
    "# lfs = [Qwen, Llama3_vision, deeplearning\n",
    "#        ]\n",
    "\n",
    "applier = LFApplier(lfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b475d082-aa6a-4fbf-8299-1b1237529a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "311it [00:02, 119.87it/s]\n",
      "8518it [01:12, 117.25it/s]\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling import LFAnalysis\n",
    "\n",
    "L_dev = applier.apply(dev_image_names)\n",
    "L_train = applier.apply(train_image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415e26e7-b9bc-4fb8-8319-695ab90e5b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "757add37-1ef5-4fc8-937f-c7b963760125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Incorrect</th>\n",
       "      <th>Emp. Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Qwen</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.874598</td>\n",
       "      <td>222</td>\n",
       "      <td>89</td>\n",
       "      <td>0.713826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama3_vision</th>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.874598</td>\n",
       "      <td>200</td>\n",
       "      <td>111</td>\n",
       "      <td>0.643087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deeplearning</th>\n",
       "      <td>2</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.874598</td>\n",
       "      <td>263</td>\n",
       "      <td>48</td>\n",
       "      <td>0.845659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>line_detection</th>\n",
       "      <td>3</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.874598</td>\n",
       "      <td>139</td>\n",
       "      <td>172</td>\n",
       "      <td>0.446945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>superpixel_detection</th>\n",
       "      <td>4</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.874598</td>\n",
       "      <td>138</td>\n",
       "      <td>173</td>\n",
       "      <td>0.443730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      j Polarity  Coverage  Overlaps  Conflicts  Correct  \\\n",
       "Qwen                  0   [0, 1]       1.0       1.0   0.874598      222   \n",
       "Llama3_vision         1   [0, 1]       1.0       1.0   0.874598      200   \n",
       "deeplearning          2   [0, 1]       1.0       1.0   0.874598      263   \n",
       "line_detection        3   [0, 1]       1.0       1.0   0.874598      139   \n",
       "superpixel_detection  4   [0, 1]       1.0       1.0   0.874598      138   \n",
       "\n",
       "                      Incorrect  Emp. Acc.  \n",
       "Qwen                         89   0.713826  \n",
       "Llama3_vision               111   0.643087  \n",
       "deeplearning                 48   0.845659  \n",
       "line_detection              172   0.446945  \n",
       "superpixel_detection        173   0.443730  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_dev = np.array(Y_dev)\n",
    "LFAnalysis(L_dev, lfs).lf_summary(Y=Y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd038dc-0729-4e6f-9f14-805066f62eb6",
   "metadata": {},
   "source": [
    "F1 of the labelers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11214f33-2540-4854-a076-f581ca68b5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, cohen_kappa_score\n",
    "import numpy as np\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, abstain_class=-1):\n",
    "    # Filter out samples where prediction is -1\n",
    "    valid_indices = y_pred != abstain_class\n",
    "    y_true_filtered = y_true[valid_indices]\n",
    "    y_pred_filtered = y_pred[valid_indices]\n",
    "\n",
    "    # Compute metrics\n",
    "    conf_matrix = confusion_matrix(y_true_filtered, y_pred_filtered)\n",
    "    precision = precision_score(y_true_filtered, y_pred_filtered)\n",
    "    recall = recall_score(y_true_filtered, y_pred_filtered)\n",
    "    f1 = f1_score(y_true_filtered, y_pred_filtered)\n",
    "    accuracy = accuracy_score(y_true_filtered, y_pred_filtered)\n",
    "    cohen_kappa = cohen_kappa_score(y_true_filtered, y_pred_filtered)\n",
    "\n",
    "    return {\n",
    "        'Confusion Matrix': conf_matrix,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Accuracy': accuracy,\n",
    "        'cohen_kappa' : cohen_kappa\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d70b98e-2e01-4b26-8456-9d4d450668b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6622be9c-eb2c-49ca-9622-98b8bea2feaa",
   "metadata": {},
   "source": [
    "Visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb7cecf-3808-4ff1-a2ff-9d463227c581",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "48d36a75-e35c-48ba-9fbf-729e8526c996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>True Negative</th>\n",
       "      <th>False Positive</th>\n",
       "      <th>False Negative</th>\n",
       "      <th>True Positive</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>cohen_kappa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Qwen</td>\n",
       "      <td>55</td>\n",
       "      <td>79</td>\n",
       "      <td>10</td>\n",
       "      <td>167</td>\n",
       "      <td>0.943503</td>\n",
       "      <td>0.678862</td>\n",
       "      <td>0.713826</td>\n",
       "      <td>0.789598</td>\n",
       "      <td>0.377566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Llama3_vision</td>\n",
       "      <td>90</td>\n",
       "      <td>44</td>\n",
       "      <td>67</td>\n",
       "      <td>110</td>\n",
       "      <td>0.621469</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.643087</td>\n",
       "      <td>0.664653</td>\n",
       "      <td>0.287124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>deeplearning</td>\n",
       "      <td>111</td>\n",
       "      <td>23</td>\n",
       "      <td>25</td>\n",
       "      <td>152</td>\n",
       "      <td>0.858757</td>\n",
       "      <td>0.868571</td>\n",
       "      <td>0.845659</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.685872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>line_detection</td>\n",
       "      <td>34</td>\n",
       "      <td>100</td>\n",
       "      <td>72</td>\n",
       "      <td>105</td>\n",
       "      <td>0.593220</td>\n",
       "      <td>0.512195</td>\n",
       "      <td>0.446945</td>\n",
       "      <td>0.549738</td>\n",
       "      <td>-0.157034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>superpixel_detection</td>\n",
       "      <td>99</td>\n",
       "      <td>35</td>\n",
       "      <td>138</td>\n",
       "      <td>39</td>\n",
       "      <td>0.220339</td>\n",
       "      <td>0.527027</td>\n",
       "      <td>0.443730</td>\n",
       "      <td>0.310757</td>\n",
       "      <td>-0.037366</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model  True Negative  False Positive  False Negative  \\\n",
       "0                  Qwen             55              79              10   \n",
       "1         Llama3_vision             90              44              67   \n",
       "2          deeplearning            111              23              25   \n",
       "3        line_detection             34             100              72   \n",
       "4  superpixel_detection             99              35             138   \n",
       "\n",
       "   True Positive    Recall  Precision  Accuracy  F1 Score  cohen_kappa  \n",
       "0            167  0.943503   0.678862  0.713826  0.789598     0.377566  \n",
       "1            110  0.621469   0.714286  0.643087  0.664653     0.287124  \n",
       "2            152  0.858757   0.868571  0.845659  0.863636     0.685872  \n",
       "3            105  0.593220   0.512195  0.446945  0.549738    -0.157034  \n",
       "4             39  0.220339   0.527027  0.443730  0.310757    -0.037366  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score, accuracy_score, f1_score\n",
    "\n",
    "# Example ground truth and predictions for six models\n",
    "# Replace these arrays with actual predictions from each model\n",
    "y_true = Y_dev\n",
    "predictions = {}\n",
    "\n",
    "for i in range(L_dev.shape[1]):\n",
    "    predictions[list_of_all_the_models[i]] = L_dev[:,i]\n",
    "    \n",
    "# Create a DataFrame to store confusion matrix results and metrics\n",
    "confusion_data = []\n",
    "\n",
    "for model_name, y_pred in predictions.items():\n",
    "    \n",
    "    y_pred = np.array(y_pred)\n",
    "    # Confusion Matrix\n",
    "    metrics = calculate_metrics(Y_dev, y_pred)\n",
    "    \n",
    "    tn, fp, fn, tp = metrics['Confusion Matrix'].ravel()\n",
    "    precision = metrics['Precision']\n",
    "    recall = metrics['Recall']\n",
    "    f1 = metrics['F1 Score']\n",
    "    accuracy = metrics['Accuracy']\n",
    "    cohen_kappa = metrics['cohen_kappa']\n",
    "    # Append data\n",
    "    confusion_data.append([\n",
    "        model_name, tn, fp, fn, tp, \n",
    "        recall, precision, accuracy, f1, cohen_kappa\n",
    "    ])\n",
    "\n",
    "# Convert to a DataFrame for display\n",
    "confusion_df = pd.DataFrame(confusion_data, columns=[\n",
    "    'Model', 'True Negative', 'False Positive', 'False Negative', 'True Positive', \n",
    "    'Recall', 'Precision', 'Accuracy', 'F1 Score', 'cohen_kappa'\n",
    "])\n",
    "\n",
    "# Display the table with confusion matrix and metrics\n",
    "confusion_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7d0ede-bab2-4e37-9c9a-75f494edd82f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e72d022c-4a6b-4258-86f8-71b615cfa8b3",
   "metadata": {},
   "source": [
    "# Optimization for Label Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "84ea9e65-8d70-4621-b3af-f13500eaaf42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:01<00:00, 3178.09epoch/s]\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling.model import LabelModel\n",
    "\n",
    "label_model = LabelModel(cardinality=2, verbose=False)\n",
    "label_model.fit(L_train, n_epochs=5000, log_freq=500, seed=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2623cd-28a2-410b-b88f-c90fb3aa1253",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "427bf3f3-3d55-40e0-938a-a9a672fb88d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[ 92  42]\n",
      " [ 44 133]]\n",
      "Precision: 0.76\n",
      "Recall: 0.751412429378531\n",
      "F1 Score: 0.7556818181818182\n",
      "Accuracy: 0.7234726688102894\n",
      "cohen_kappa: 0.4371869870796684\n"
     ]
    }
   ],
   "source": [
    "from snorkel.analysis import metric_score\n",
    "from snorkel.utils import probs_to_preds\n",
    "\n",
    "probs_dev = label_model.predict_proba(L_dev)\n",
    "preds_dev = probs_to_preds(probs_dev)\n",
    "\n",
    "metrics = calculate_metrics(Y_dev, preds_dev)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b524bc2-16fc-468d-a61a-f7a252372429",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f93639-7912-4295-84ef-06dada1d4e94",
   "metadata": {},
   "source": [
    "Experimenting with different number of LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3cb0df3-92f7-4550-a8aa-0ec8c0715120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 3724.30epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[ 91  43]\n",
      " [ 19 158]]\n",
      "Precision: 0.7860696517412935\n",
      "Recall: 0.8926553672316384\n",
      "F1 Score: 0.8359788359788359\n",
      "Accuracy: 0.8006430868167203\n",
      "cohen_kappa: 0.5844754762520472\n",
      "trained with the LFs = ['Qwen', 'Llama3_vision', 'deeplearning']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_f1s = []\n",
    "number_of_epochs = 100\n",
    "for i in range(3, L_train.shape[1]+1):\n",
    "\n",
    "    \n",
    "    label_model = LabelModel(cardinality=2, verbose=False)\n",
    "    label_model.fit(L_train[:,:i], Y_dev, n_epochs=number_of_epochs, log_freq=500, seed=12)\n",
    "    \n",
    "    probs_dev = label_model.predict_proba(L_dev[:,:i])\n",
    "    preds_dev = probs_to_preds(probs_dev)\n",
    "\n",
    "    metrics = calculate_metrics(Y_dev, preds_dev)\n",
    "    all_f1s.append(metrics['F1 Score'])\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "    print(f'trained with the LFs = {list_of_all_the_models[:i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeee78a-8ef5-463e-b837-01e5e2399848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = np.array([4, 5, 6])\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x, all_f1s, marker='s', linestyle='-', color='b')\n",
    "plt.title(f'The Label Model Performance After Optimizing For {number_of_epochs} Iterations')\n",
    "plt.xlabel('Number of LFs')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.grid(True)\n",
    "# plt.savefig('HM_LM_1000_4-7.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389a9722-eb19-4d62-b6ce-ff6824cfb782",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_f1s = []\n",
    "# number_of_epochs = [10, 20, 50, 80, 100, 500, 1000, 5000]\n",
    "number_of_epochs = range(1,100)\n",
    "for epochs in number_of_epochs:\n",
    "\n",
    "    \n",
    "    label_model = LabelModel(cardinality=2, verbose=False)\n",
    "    label_model.fit(L_train, Y_dev, n_epochs=epochs, log_freq=500, seed=12)\n",
    "    \n",
    "    probs_dev = label_model.predict_proba(L_dev)\n",
    "    preds_dev = probs_to_preds(probs_dev)\n",
    "\n",
    "    metrics = calculate_metrics(Y_dev, preds_dev)\n",
    "    all_f1s.append(metrics['F1 Score'])\n",
    "    # for metric, value in metrics.items():\n",
    "    #     print(f\"{metric}: {value}\")\n",
    "    # print(f'trained with the LFs = {list_of_all_the_models[:i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b73fc8e-0d4c-4cbe-9205-a91d4dedae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = np.array(number_of_epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x, all_f1s, marker='s', linestyle='-', color='b')\n",
    "plt.title(f'The Label Model Performance After Optimizing For {number_of_epochs} Iterations')\n",
    "plt.xlabel('Number of LFs')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.grid(True)\n",
    "# plt.savefig('HM_LM_1000_4-7.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9cf21b-47ee-48ec-878a-3ceea27d9cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b362ac-22db-4739-8a28-db45a372e707",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_f1s[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c11aac3-a236-48ed-b80f-f97875351d15",
   "metadata": {},
   "source": [
    "# Training the End Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85550ff2-272c-4f20-acba-1cddf2ad952b",
   "metadata": {},
   "source": [
    "The dataloader for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90e13508-d0fb-4f2c-89da-98c7961f60ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Custom dataset class for loading images\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_names, root_dir, labels, target_dists, val=False, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_names (list): List of image file names.\n",
    "            root_dir (string): Directory where images are stored.\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.data = []\n",
    "        self.tile_numbers = []\n",
    "        self.val = val\n",
    "        \n",
    "        self.image_names = image_names\n",
    "        self.root_dir = root_dir\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.target_dists = target_dists\n",
    "        tile_files = [f for f in os.listdir(self.root_dir) if f.endswith('.tif')]\n",
    "        # neg_files = [f for f in os.listdir(neg_dir) if f.endswith('.tif')]\n",
    "        tile_dict = self.group_files_by_tile(tile_files)\n",
    "        \n",
    "        self.store_tiles(tile_dict, self.root_dir)\n",
    "\n",
    "    \n",
    "    def group_files_by_tile(self, files):\n",
    "        tile_dict = {}\n",
    "        # print(len(files))\n",
    "        for file in files:\n",
    "            tile_number = file.split('_')[-1].split('.')[0]\n",
    "            if tile_number not in tile_dict:\n",
    "                tile_dict[tile_number] = []\n",
    "            tile_dict[tile_number].append(file)\n",
    "        # Only include complete groups\n",
    "        # return [tile for tile in tile_dict.values() if len(tile) == 6]\n",
    "        # return [tile for tile in tile_dict.values()]\n",
    "        # print(tile_dict)\n",
    "        return tile_dict\n",
    "\n",
    "    def store_tiles(self, tile_dict, directory):\n",
    "        for tile_number in self.image_names:\n",
    "            # print(\"----\")\n",
    "            # print(tile_number)\n",
    "           \n",
    "            self.data.append([os.path.join(directory, f) for f in sorted(tile_dict[tile_number])])\n",
    "\n",
    "            # else:\n",
    "            #     self.data.append([os.path.join(directory, f) for f in sorted(tile_dict[])])\n",
    "            self.tile_numbers.append(tile_number)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Build the full path of the image file\n",
    "        # img_name = os.path.join(self.root_dir, self.image_names[idx])\n",
    "        # label = self.labels[idx]\n",
    "        # target_dist = self.target_dists[idx]\n",
    "        # image = Image.open(img_name).convert('RGB')  # Load image as RGB\n",
    "\n",
    "        # # Apply any transformations (e.g., resize, normalization)\n",
    "        # if self.transform:\n",
    "        #     image = self.transform(image)\n",
    "        \n",
    "        image_paths = self.data[idx]\n",
    "        images_1 = [imageio.imread(img_path).astype('uint8') for img_path in self.data[idx]]\n",
    "        images = [transforms.functional.to_pil_image(image) for image in images_1]\n",
    "        if self.transform:\n",
    "            images = [self.transform(image) for image in images]\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        target_dist = self.target_dists[idx]\n",
    "        \n",
    "        tile_number = self.tile_numbers[idx]\n",
    "        # label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return images, torch.tensor(label), torch.tensor(target_dist), tile_number, image_paths\n",
    "\n",
    "        # return image, torch.tensor(label), torch.tensor(target_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f892c82d-e848-41b8-9c58-98462f84401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root_dir = \"/home/macula/SMATousi/Gullies/ground_truth/organized_data/All_Pos_Neg/combined_folder_true_rgb/\"\n",
    "validation_root_dir = \"/home/macula/SMATousi/cluster/docker-images/ollama/MO+IA_test_data_numbered/\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize images to 224x224 (example)\n",
    "        transforms.ToTensor(),          # Convert images to PyTorch tensors\n",
    "        # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet mean/std\n",
    "    ])\n",
    "\n",
    "probs_dev = label_model.predict_proba(L_train)\n",
    "label_model_predictions = probs_to_preds(probs_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "67f46450-1202-464e-bb6e-24afd3af5653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['415', '1020', '105', '439', '914', '1099', '1065', '373', '166', '396', '837', '685', '226', '956', '70', '604', '1067', '118', '774', '521', '910', '975', '352', '761', '1007', '428', '1038', '50', '838', '126', '1078', '944', '532', '1041', '540', '128', '722', '478', '639', '668', '343', '1021', '552', '848', '74', '520', '1032', '615', '536', '1117', '646', '390', '923', '194', '216', '99', '372', '857', '335', '505', '972', '727', '1064', '740', '182', '316', '706', '193', '882', '932', '174', '440', '965', '1140', '273', '878', '826', '596', '1123', '1031', '942', '239', '1005', '139', '11', '855', '836', '97', '124', '568', '387', '171', '644', '695', '12', '739', '732', '1035', '242', '441', '928', '71', '248', '794', '458', '915', '475', '1050', '546', '556', '619', '721', '614', '626', '404', '759', '334', '486', '153', '5', '1010', '272', '219', '125', '222', '1098', '758', '213', '235', '916', '253', '573', '636', '1053', '927', '398', '231', '502', '410', '4', '895', '344', '202', '399', '724', '534', '225', '669', '538', '306', '840', '190', '418', '337', '541', '930', '411', '529', '447', '701', '1085', '513', '1073', '820', '224', '551', '312', '847', '476', '250', '129', '579', '742', '10', '392', '377', '950', '454', '686', '569', '236', '1004', '19', '207', '301', '819', '887', '666', '789', '867', '967', '752', '691', '474', '854', '237', '28', '164', '1043', '1137', '597', '449', '643', '542', '397', '834', '1138', '784', '1062', '918', '788', '1143', '168', '866', '83', '1040', '357', '670', '473', '100', '812', '515', '52', '1121', '416', '907', '107', '519', '748', '289', '471', '1141', '175', '1114', '287', '608', '681', '320', '266', '36', '593', '310', '421', '244', '605', '1017', '485', '179', '209', '358', '59', '711', '355', '851', '158', '1095', '797', '331', '831', '723', '123', '1051', '955', '966', '852', '160', '522', '319', '911', '430', '208', '1081', '135', '881', '511', '991', '719', '90', '350', '616', '631', '999', '945', '766', '842', '359', '629', '1049', '117', '859', '549', '1015', '98', '557', '104', '898', '322', '288', '1128', '163', '903', '137', '251', '865', '391', '267', '1018', '973', '212', '131', '1133']\n"
     ]
    }
   ],
   "source": [
    "dev_image_names_modified = [entry.split('-')[-1] for entry in dev_image_names]\n",
    "print(dev_image_names_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "03c0e90b-c7be-44cf-bfe3-01a361aa2291",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageDataset(image_names=train_image_names, \n",
    "                       root_dir=root_dir, \n",
    "                       labels=label_model_predictions, \n",
    "                       target_dists=probs_dev, \n",
    "                       transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "\n",
    "dev_dataset = ImageDataset(image_names=dev_image_names_modified, \n",
    "                           root_dir=validation_root_dir, \n",
    "                           labels=Y_dev, \n",
    "                           target_dists=probs_dev,\n",
    "                           transform=transform)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=8, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c23b909-cf72-49f2-b684-9c46346df43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(dataloader)\n",
    "\n",
    "images, label, target_dists, tile_number, image_paths = next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae43b0b3-abec-4500-a90b-2aff295d1e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e681e5-63f2-4f8c-9356-07444d18f504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7bf71dc2-729a-4ba4-b8f4-33e438edc863",
   "metadata": {},
   "source": [
    "A basic ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b4f13e49-02cb-4a0d-9e38-ebe167e96c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def expected_cross_entropy_loss(logits, target_distributions):\n",
    "    \"\"\"\n",
    "    Computes the expected cross-entropy loss for a batch of predictions and target distributions.\n",
    "\n",
    "    Parameters:\n",
    "    logits (torch.Tensor): The raw output from the model of shape (batch_size, num_classes).\n",
    "    target_distributions (torch.Tensor): The target class distributions of shape (batch_size, num_classes),\n",
    "                                         where each row is a probability distribution over classes.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: The expected cross-entropy loss.\n",
    "    \"\"\"\n",
    "    # Convert logits to log probabilities\n",
    "    log_probs = F.log_softmax(logits, dim=1)\n",
    "    \n",
    "    # Compute the element-wise product between target distributions and log probabilities\n",
    "    # Then, sum across classes to get the cross-entropy for each instance\n",
    "    cross_entropy = -torch.sum(target_distributions * log_probs, dim=1)\n",
    "    \n",
    "    # Take the mean over the batch\n",
    "    loss = cross_entropy.mean()\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dfac98-4198-4130-9117-060ff4c51175",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0bcf001b-a515-4e9f-8d4e-2bf5114600b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def calculate_metrics(y_true, y_pred, abstain_class=-1):\n",
    "    # Filter out samples where prediction is -1\n",
    "    # valid_indices = y_pred != abstain_class\n",
    "    # y_true_filtered = y_true[valid_indices]\n",
    "    # y_pred_filtered = y_pred[valid_indices]\n",
    "\n",
    "    # Compute metrics\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        'Confusion Matrix': conf_matrix,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Accuracy': accuracy\n",
    "    }\n",
    "\n",
    "\n",
    "# Function to evaluate the model on the dev set\n",
    "def evaluate(model, dev_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for images, labels, target_dist, _, _ in tqdm(dev_loader):\n",
    "            labels =  labels.to(device)\n",
    "            list_of_images = [image.to(device) for image in images]\n",
    "\n",
    "            # Forward pass to get outputs\n",
    "            outputs = model(list_of_images)\n",
    "\n",
    "            # Get predictions (class with the highest score)\n",
    "            # _, predicted = torch.max(outputs.data, 1)\n",
    "            predicted = torch.round(outputs)\n",
    "\n",
    "            # Store true labels and predictions\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "    return np.array(all_labels), np.array(all_preds)\n",
    "    \n",
    "def train(model, train_loader, val_loader, criterion, optimizer, device, epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels, target_dist, _, _ in tqdm(train_loader):\n",
    "            labels, target_dist = labels.to(device), target_dist.to(device)\n",
    "            # print(labels)\n",
    "            list_of_images = [image.to(device) for image in images]\n",
    "\n",
    "            optimizer.zero_grad()  # Clear the gradients\n",
    "\n",
    "            outputs = model(list_of_images)  # Forward pass\n",
    "            # print(outputs)\n",
    "            \n",
    "            # print(outputs)\n",
    "            # print(outputs.dtype)\n",
    "            # print(labels)\n",
    "            # probs, predicted = torch.max(outputs.data, 1)\n",
    "            # max_prob = torch.max(probs)\n",
    "            loss = criterion(outputs.squeeze(), labels.float())  # Compute the loss\n",
    "            # loss = expected_cross_entropy_loss(outputs, target_dist)\n",
    "\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update the weights\n",
    "\n",
    "            # Calculate running loss and accuracy\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            predicted = torch.round(outputs.data)\n",
    "            # print(predicted)\n",
    "            \n",
    "            # print(predicted)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            # break\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "        labels, predictions = evaluate(model, val_loader, device)\n",
    "        # print(labels)\n",
    "        # print(predictions)\n",
    "\n",
    "        # Calculate and print precision, recall, and F1-score\n",
    "        # calculate_metrics(labels, predictions)\n",
    "        metrics = calculate_metrics(labels, predictions)\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "85337a25-23c3-4bdb-98c6-3f921342c85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/macula/SMATousi/.conda/envs/snorkel/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/macula/SMATousi/.conda/envs/snorkel/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "  0%|                                                                                                                   | 0/1065 [00:00<?, ?it/s]/tmp/ipykernel_3257828/1289081151.py:72: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  images_1 = [imageio.imread(img_path).astype('uint8') for img_path in self.data[idx]]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 1065/1065 [17:30<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.5384, Accuracy: 406.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 39/39 [00:33<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[ 94  40]\n",
      " [ 52 125]]\n",
      "Precision: 0.7575757575757576\n",
      "Recall: 0.7062146892655368\n",
      "F1 Score: 0.7309941520467836\n",
      "Accuracy: 0.7041800643086816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                   | 0/1065 [00:00<?, ?it/s]/tmp/ipykernel_3257828/1289081151.py:72: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  images_1 = [imageio.imread(img_path).astype('uint8') for img_path in self.data[idx]]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 1065/1065 [17:06<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 0.6944, Accuracy: 420.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 39/39 [00:26<00:00,  1.47it/s]\n",
      "/home/macula/SMATousi/.conda/envs/snorkel/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[134   0]\n",
      " [177   0]]\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n",
      "Accuracy: 0.43086816720257237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                   | 0/1065 [00:00<?, ?it/s]/tmp/ipykernel_3257828/1289081151.py:72: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  images_1 = [imageio.imread(img_path).astype('uint8') for img_path in self.data[idx]]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 1065/1065 [16:10<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 0.6635, Accuracy: 424.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 39/39 [00:32<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[ 40  94]\n",
      " [ 39 138]]\n",
      "Precision: 0.5948275862068966\n",
      "Recall: 0.7796610169491526\n",
      "F1 Score: 0.6748166259168704\n",
      "Accuracy: 0.572347266881029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                   | 0/1065 [00:00<?, ?it/s]/tmp/ipykernel_3257828/1289081151.py:72: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  images_1 = [imageio.imread(img_path).astype('uint8') for img_path in self.data[idx]]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 1065/1065 [16:13<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.5372, Accuracy: 427.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 39/39 [00:30<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[ 72  62]\n",
      " [ 41 136]]\n",
      "Precision: 0.6868686868686869\n",
      "Recall: 0.768361581920904\n",
      "F1 Score: 0.7253333333333334\n",
      "Accuracy: 0.6688102893890675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                   | 0/1065 [00:00<?, ?it/s]/tmp/ipykernel_3257828/1289081151.py:72: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  images_1 = [imageio.imread(img_path).astype('uint8') for img_path in self.data[idx]]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 1065/1065 [16:01<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.4877, Accuracy: 427.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 39/39 [00:29<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[ 56  78]\n",
      " [ 38 139]]\n",
      "Precision: 0.6405529953917051\n",
      "Recall: 0.7853107344632768\n",
      "F1 Score: 0.7055837563451777\n",
      "Accuracy: 0.6270096463022508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                   | 0/1065 [00:00<?, ?it/s]/tmp/ipykernel_3257828/1289081151.py:72: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  images_1 = [imageio.imread(img_path).astype('uint8') for img_path in self.data[idx]]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 1065/1065 [16:34<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.4527, Accuracy: 431.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 39/39 [00:35<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[ 89  45]\n",
      " [ 54 123]]\n",
      "Precision: 0.7321428571428571\n",
      "Recall: 0.6949152542372882\n",
      "F1 Score: 0.7130434782608696\n",
      "Accuracy: 0.6816720257234726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                   | 0/1065 [00:00<?, ?it/s]/tmp/ipykernel_3257828/1289081151.py:72: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  images_1 = [imageio.imread(img_path).astype('uint8') for img_path in self.data[idx]]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 1065/1065 [15:53<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 0.4248, Accuracy: 432.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 39/39 [00:31<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[ 91  43]\n",
      " [ 69 108]]\n",
      "Precision: 0.7152317880794702\n",
      "Recall: 0.6101694915254238\n",
      "F1 Score: 0.6585365853658537\n",
      "Accuracy: 0.639871382636656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                   | 0/1065 [00:00<?, ?it/s]/tmp/ipykernel_3257828/1289081151.py:72: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  images_1 = [imageio.imread(img_path).astype('uint8') for img_path in self.data[idx]]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 1065/1065 [17:12<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.3873, Accuracy: 433.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 39/39 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[ 66  68]\n",
      " [ 33 144]]\n",
      "Precision: 0.6792452830188679\n",
      "Recall: 0.8135593220338984\n",
      "F1 Score: 0.7403598971722365\n",
      "Accuracy: 0.6752411575562701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                   | 0/1065 [00:00<?, ?it/s]/tmp/ipykernel_3257828/1289081151.py:72: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  images_1 = [imageio.imread(img_path).astype('uint8') for img_path in self.data[idx]]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 1065/1065 [16:27<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 0.3595, Accuracy: 436.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 39/39 [00:30<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[ 65  69]\n",
      " [ 22 155]]\n",
      "Precision: 0.6919642857142857\n",
      "Recall: 0.8757062146892656\n",
      "F1 Score: 0.773067331670823\n",
      "Accuracy: 0.707395498392283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                   | 0/1065 [00:00<?, ?it/s]/tmp/ipykernel_3257828/1289081151.py:72: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  images_1 = [imageio.imread(img_path).astype('uint8') for img_path in self.data[idx]]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 1065/1065 [17:28<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.3204, Accuracy: 439.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 39/39 [00:27<00:00,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[ 82  52]\n",
      " [ 40 137]]\n",
      "Precision: 0.7248677248677249\n",
      "Recall: 0.7740112994350282\n",
      "F1 Score: 0.7486338797814208\n",
      "Accuracy: 0.7041800643086816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = 'cpu'\n",
    "\n",
    "resnet_extractor = ResNetFeatureExtractor()\n",
    "mlp_classifier = MLPClassifier(input_size=6*2048, hidden_size=512, output_size=1)\n",
    "\n",
    "model = Gully_Classifier(input_size=6*2048, hidden_size=512, output_size=1).to(device)\n",
    "# model = Gully_Classifier(input_size=6*2048, hidden_size=512, output_size=1)\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()  # Binary classification loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Only optimize the MLP parameters\n",
    "\n",
    "# Train the model\n",
    "epochs = 10\n",
    "train(model, dataloader, dev_dataloader, criterion, optimizer, device, epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cea2d27-e855-481d-9df1-490cf74feb7d",
   "metadata": {},
   "source": [
    "Evaluation ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7db26a7-aa93-4d22-b17b-dad2a26d7344",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd333283-c836-4eac-b7f3-44892acf6a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Function to evaluate the model on the dev set\n",
    "def evaluate(model, dev_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for images, labels, target_dist in tqdm(dev_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass to get outputs\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Get predictions (class with the highest score)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # Store true labels and predictions\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "    return np.array(all_labels), np.array(all_preds)\n",
    "\n",
    "# Function to calculate precision, recall, and F1 score\n",
    "def calculate_metrics(labels, predictions):\n",
    "    precision = precision_score(labels, predictions)\n",
    "    recall = recall_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions)\n",
    "    \n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7526a040-0b04-4da3-8289-ffb05d17a3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset = ImageDataset(image_names=dev_image_names, root_dir=root_dir, labels=Y_dev, target_dists=probs_dev, transform=transform)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=8, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0443e3-46ec-44ac-af0c-06375b1924b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, predictions = evaluate(model, dev_dataloader, device)\n",
    "\n",
    "# Calculate and print precision, recall, and F1-score\n",
    "# calculate_metrics(labels, predictions)\n",
    "metrics = calculate_metrics(labels, predictions)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca419d4-c42f-4c8f-a056-1d222c5a656f",
   "metadata": {},
   "source": [
    "Training the CLIP+MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63738088-381e-45ee-9d8b-cf80e44b11bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
