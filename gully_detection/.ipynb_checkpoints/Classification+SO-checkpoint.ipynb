{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78c63dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import wandb\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import DistributedDataParallelKwargs\n",
    "import rasterio\n",
    "from model import *\n",
    "from dataset import *\n",
    "from utils import *\n",
    "import sys\n",
    "import rasterio\n",
    "from rasterio.merge import merge\n",
    "from rasterio.plot import show\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "import importlib.util\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the module's directory to the sys.path\n",
    "module_dir = Path('../dem2so/').parent / \"dem2so\"\n",
    "sys.path.append(str(module_dir))\n",
    "\n",
    "# Import the module\n",
    "file1_spec = importlib.util.spec_from_file_location(\"model\", module_dir / \"model.py\")\n",
    "file1 = importlib.util.module_from_spec(file1_spec)\n",
    "file1_spec.loader.exec_module(file1)\n",
    "\n",
    "file2_spec = importlib.util.spec_from_file_location(\"dataset\", module_dir / \"dataset.py\")\n",
    "file2 = importlib.util.module_from_spec(file2_spec)\n",
    "file2_spec.loader.exec_module(file2)\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b016a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mstmmc\u001b[0m (\u001b[33mtousi-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/macula/SMATousi/Gullies/ground_truth/google_api/training_process/Gully_Detection/gully_detection/wandb/run-20240722_180727-x00wn8eq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tousi-team/RGB_DEM_2_SO/runs/x00wn8eq' target=\"_blank\">comfy-sun-52</a></strong> to <a href='https://wandb.ai/tousi-team/RGB_DEM_2_SO' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tousi-team/RGB_DEM_2_SO' target=\"_blank\">https://wandb.ai/tousi-team/RGB_DEM_2_SO</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tousi-team/RGB_DEM_2_SO/runs/x00wn8eq' target=\"_blank\">https://wandb.ai/tousi-team/RGB_DEM_2_SO/runs/x00wn8eq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"RGB_DEM_2_SO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edf06642",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact model_epoch_400:v5, 144.44MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:3.8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "artifact = run.use_artifact('tousi-team/RGB_DEM_2_SO/model_epoch_400:v5', type='model')\n",
    "artifact_dir = artifact.download(\"./artifacts/models/dem2so/old_models/model_epoch_400/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "563df336",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGB_DEM_to_SO(nn.Module):\n",
    "    def __init__(self, resnet_output_size, \n",
    "                 fusion_output_size, \n",
    "                 model_choice, \n",
    "                 resnet_saved_model_path,\n",
    "                 input_choice='RD', \n",
    "                 dropout_rate=0.5,\n",
    "                 number_of_in_channels=2):\n",
    "        \n",
    "        super(RGB_DEM_to_SO, self).__init__()\n",
    "        self.resnet = file1.ResNetFeatures(output_size=resnet_output_size, saved_model_path=resnet_saved_model_path)\n",
    "        self.fusion_net = file1.FusionNet(input_channels=6*2048, output_size=fusion_output_size)\n",
    "        self.unet = file1.UNet_1(n_channels=number_of_in_channels, n_classes=9, dropout_rate=dropout_rate)\n",
    "        self.unet_light = file1.UNet_light(n_channels=number_of_in_channels, n_classes=9, dropout_rate=dropout_rate)\n",
    "#         self.onet = file1.BothNet(in_channels=number_of_in_channels, out_channels=9)\n",
    "        self.model_choice = model_choice\n",
    "        self.input_choice = input_choice\n",
    "\n",
    "    def forward(self, dem, rgbs):\n",
    "\n",
    "        # rgbs is a list of RGB images\n",
    "        features = [self.resnet(rgb) for rgb in rgbs]\n",
    "        features = torch.cat(features, dim=1)  # Concatenate features along the channel dimension\n",
    "        fused = self.fusion_net(features)\n",
    "\n",
    "        # Concatenate DEM and fused features\n",
    "        combined_input = torch.cat((dem, fused), dim=1)\n",
    "        if self.model_choice == \"Unet_1\":\n",
    "            so_output = self.unet(combined_input)\n",
    "        if self.model_choice == \"Unet_light\":\n",
    "            so_output = self.unet_light(combined_input)\n",
    "        if self.model_choice == \"Onet\":\n",
    "            so_output = self.onet(combined_input)\n",
    "  \n",
    "\n",
    "        return so_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d511664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tiffs(input_folder, output_filepath):\n",
    "    \"\"\"\n",
    "    Merge multiple GeoTIFF files into a single larger TIFF file.\n",
    "\n",
    "    :param input_folder: Folder containing all TIFF files to merge.\n",
    "    :param output_filepath: Path to save the merged TIFF file.\n",
    "    \"\"\"\n",
    "    # Search for TIFF files in the folder\n",
    "    search_criteria = \"*.tif\"\n",
    "    query = os.path.join(input_folder, search_criteria)\n",
    "    tif_files = glob.glob(query)\n",
    "\n",
    "    # List to hold open datasets\n",
    "    src_files_to_mosaic = []\n",
    "\n",
    "    # Open and append each TIFF file to the list\n",
    "    for filepath in tif_files:\n",
    "        src = rasterio.open(filepath)\n",
    "        src_files_to_mosaic.append(src)\n",
    "\n",
    "    # Merge function from rasterio\n",
    "    mosaic, out_trans = merge(src_files_to_mosaic)\n",
    "\n",
    "    # Copy the metadata\n",
    "    out_meta = src_files_to_mosaic[0].meta.copy()\n",
    "\n",
    "    # Update the metadata to reflect the number of layers\n",
    "    out_meta.update({\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"height\": mosaic.shape[1],\n",
    "        \"width\": mosaic.shape[2],\n",
    "        \"transform\": out_trans,\n",
    "        \"crs\": src_files_to_mosaic[0].crs\n",
    "    })\n",
    "\n",
    "    # Write the mosaic raster to the new file\n",
    "    with rasterio.open(output_filepath, \"w\", **out_meta) as dest:\n",
    "        for i in range(1, mosaic.shape[0]+1):\n",
    "            dest.write(mosaic[i-1], i)\n",
    "\n",
    "    # Close all rasterio opened files\n",
    "    for src in src_files_to_mosaic:\n",
    "        src.close()\n",
    "\n",
    "    print(\"Merge completed successfully. Output saved at:\", output_filepath)\n",
    "    \n",
    "class RGB_RasterTilesDataset_Geo(Dataset):\n",
    "    def __init__(self, dem_dir, so_dir, rgb_dir, transform=None):\n",
    "        self.dem_dir = dem_dir\n",
    "        self.so_dir = so_dir\n",
    "        self.rgb_dir = rgb_dir\n",
    "        self.transform = transform\n",
    "        # Assume all DEM, SO, and RGB files share the same tile identifiers\n",
    "        self.tile_identifiers = [f.split('_')[-1].split('.')[0] for f in os.listdir(dem_dir) if 'dem_tile' in f]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tile_identifiers)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        tile_id = self.tile_identifiers[idx]\n",
    "        dem_file = os.path.join(self.dem_dir, f'dem_tile_{tile_id}.tif')\n",
    "        so_file = os.path.join(self.so_dir, f'dem_tile_{tile_id}.tif')\n",
    "        rgb_files = [os.path.join(self.rgb_dir, f'rgb{k}_tile_{tile_id}.tif') for k in range(6)]\n",
    "\n",
    "        # Prepare sample dictionary\n",
    "        sample = {}\n",
    "\n",
    "        # Read DEM file and extract the transform\n",
    "        with rasterio.open(dem_file) as src:\n",
    "            dem_image = src.read(1)  # Read the first band\n",
    "            dem_transform = src.transform\n",
    "            sample['DEM'] = dem_image\n",
    "            sample['DEM_transform'] = dem_transform\n",
    "\n",
    "        # Read SO file and extract the transform\n",
    "        with rasterio.open(so_file) as src:\n",
    "            so_image = src.read(1)\n",
    "            so_transform = src.transform\n",
    "            sample['SO'] = so_image\n",
    "            sample['SO_transform'] = so_transform\n",
    "\n",
    "        # Read RGB files and extract their transforms\n",
    "        rgb_images = []\n",
    "        rgb_transforms = []\n",
    "        for file in rgb_files:\n",
    "            with rasterio.open(file) as src:\n",
    "                rgb_images.append(src.read([1, 2, 3]))  # Read the RGB bands\n",
    "                rgb_transforms.append(src.transform)\n",
    "        sample['RGB'] = rgb_images\n",
    "        sample['RGB_transforms'] = rgb_transforms\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fa293cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46f8e90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio.transform import from_origin\n",
    "\n",
    "def save_as_geotiff(data, profile, output_path, single_value=False, image_size=128):\n",
    "    \"\"\"\n",
    "    Save a numpy array or single value as a GeoTIFF file with a given raster profile.\n",
    "    \n",
    "    :param data: Numpy array to save (2D or 3D array where 3D includes bands) or a single value.\n",
    "    :param profile: Dictionary containing raster metadata like crs, transform, etc.\n",
    "    :param output_path: Path where the GeoTIFF file will be saved\n",
    "    \"\"\"\n",
    "    # Check if data is a single value and create a 2D array with the same value\n",
    "    if single_value:\n",
    "\n",
    "        data = np.full((image_size, image_size), data, dtype=profile.get('dtype', 'float32'))\n",
    "\n",
    "    # Update the profile to accommodate the data dimensions\n",
    "    profile.update({\n",
    "        'dtype': data.dtype,\n",
    "        'height': data.shape[0],\n",
    "        'width': data.shape[1],\n",
    "        'count': 1 if data.ndim == 2 else data.shape[0],  # Assuming bands are the first dimension if 3D\n",
    "        'driver': 'GTiff',\n",
    "        'nodata': None  # Set this to the appropriate nodata value if required\n",
    "    })\n",
    "#     print(profile)\n",
    "\n",
    "    # Save the data to a GeoTIFF file\n",
    "    with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "        if data.ndim == 2:\n",
    "#             print('2')\n",
    "            dst.write(data, 1)  # Write data as the first band\n",
    "        else:\n",
    "            for i in range(data.shape[0]):\n",
    "                dst.write(data[i], i + 1)  # Write each band data\n",
    "                \n",
    "def calculate_stats(folder_path):\n",
    "    means = []\n",
    "    stds = []\n",
    "    \n",
    "    # Loop through all files in the directory\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(\".tif\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            # Open the TIFF file\n",
    "            with rasterio.open(file_path) as src:\n",
    "                # Read data, assuming it's a single band\n",
    "                array = src.read(1)\n",
    "                # Combine conditions for NoData and zero values\n",
    "                if src.nodata is not None:\n",
    "                    mask = (array != src.nodata) & (array != 0)\n",
    "                else:\n",
    "                    mask = (array != 0)\n",
    "                \n",
    "                # Apply mask\n",
    "                valid_data = array[mask]\n",
    "                \n",
    "                # Calculate mean and std dev and append to lists if valid data exists\n",
    "                if valid_data.size > 0:\n",
    "                    means.append(np.mean(valid_data))\n",
    "                    stds.append(np.std(valid_data))\n",
    "                else:\n",
    "                    print(f\"Warning: No valid data in file {filename} after masking. Skipping statistics.\")\n",
    "\n",
    "    # Calculate overall statistics\n",
    "    overall_mean = np.mean(means) if means else 0\n",
    "    overall_std = np.mean(stds) if stds else 0\n",
    "\n",
    "    return overall_mean, overall_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "947194a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_name = 'Gotman'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da866823",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Mean: 226.09827\n",
      "Overall Standard Deviation: 1.8296707\n",
      "cuda\n",
      "3696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3696/3696 [17:18<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge completed successfully. Output saved at: /home/macula/SMATousi/Gullies/ground_truth/organized_data/tiled_HUCs/HUC_070801030408/merged_so+classification_results.tif\n"
     ]
    }
   ],
   "source": [
    "# dem_dir = f'/home/macula/SMATousi/Gullies/SO_Paper/data/{field_name}/dem/'\n",
    "# so_dir = f'/home/macula/SMATousi/Gullies/SO_Paper/data/{field_name}/dem/'\n",
    "# rgb_dir = f'/home/macula/SMATousi/Gullies/SO_Paper/data/{field_name}/rgb/'\n",
    "# pretrained_model_path = '/home/macula/SMATousi/cluster/docker-images/dem2so_more_data/pre_models/B3_rn50_moco_0099_ckpt.pth'\n",
    "\n",
    "dem_dir = f'/home/macula/SMATousi/Gullies/ground_truth/organized_data/tiled_HUCs/HUC_070801030408/dem/'\n",
    "so_dir = f'/home/macula/SMATousi/Gullies/ground_truth/organized_data/tiled_HUCs/HUC_070801030408/dem/'\n",
    "rgb_dir = f'/home/macula/SMATousi/Gullies/ground_truth/organized_data/tiled_HUCs/HUC_070801030408/rgb/'\n",
    "pretrained_model_path = '/home/macula/SMATousi/cluster/docker-images/dem2so_more_data/pre_models/B3_rn50_moco_0099_ckpt.pth'\n",
    "\n",
    "\n",
    "mean, std = calculate_stats(dem_dir)\n",
    "print(\"Overall Mean:\", mean)\n",
    "print(\"Overall Standard Deviation:\", std)\n",
    "\n",
    "class RGB_RasterTransform_Geo:\n",
    "    \"\"\"\n",
    "    A custom transform class for raster data.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        dem, so, rgb = sample['DEM'], sample['SO'], sample['RGB']\n",
    "        dem_meta, so_meta, rgb_meta = sample['DEM_transform'], sample['SO_transform'], sample['RGB_transforms']\n",
    "\n",
    "        # Random horizontal flipping\n",
    "        # if torch.rand(1) > 0.5:\n",
    "        #     dem = TF.hflip(dem)\n",
    "        #     so = TF.hflip(so)\n",
    "\n",
    "        # # Random vertical flipping\n",
    "        # if torch.rand(1) > 0.5:\n",
    "        #     dem = TF.vflip(dem)\n",
    "        #     so = TF.vflip(so)\n",
    "\n",
    "        # Convert numpy arrays to tensors\n",
    "        dem = TF.to_tensor(dem)\n",
    "        so = TF.to_tensor(so)\n",
    "        rgb_images = [TF.to_tensor(image) for image in rgb]\n",
    "        float_rgb_images = [image.float() for image in rgb_images]\n",
    "        # rgb_images = rgb_images.float()\n",
    "\n",
    "        dem = TF.normalize(dem, mean, std)\n",
    "\n",
    "        so = so.long()\n",
    "\n",
    "        return {'DEM': dem, 'SO': so.squeeze(), 'RGB': float_rgb_images,\n",
    "                'DEM_transform' : dem_meta, 'SO_transform' : so_meta, 'RGB_transforms' : rgb_meta}\n",
    "\n",
    "transform = RGB_RasterTransform_Geo()\n",
    "    \n",
    "dataset = RGB_RasterTilesDataset_Geo(dem_dir=dem_dir, so_dir=so_dir, rgb_dir=rgb_dir, transform=transform)\n",
    "# dataset = RGB_RasterTilesDataset_Geo(dem_dir=dem_dir, so_dir=so_dir, rgb_dir=rgb_dir)\n",
    "\n",
    "batch_size = 1\n",
    "learning_rate = 0.0001\n",
    "epochs = 1\n",
    "number_of_workers = 0\n",
    "image_size = 128\n",
    "val_percent = 0.0\n",
    "\n",
    "n_val = int(len(dataset) * val_percent)\n",
    "n_train = len(dataset) - n_val\n",
    "train, val = random_split(dataset, [n_train, n_val])\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=False, num_workers=number_of_workers, pin_memory=True)\n",
    "val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, num_workers=number_of_workers, pin_memory=True, drop_last=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)\n",
    "\n",
    "classification_model = Gully_Classifier(input_size=6*2048, hidden_size=512, output_size=1).to(device)\n",
    "\n",
    "SO_detection_model = file1.RGB_DEM_to_SO(resnet_output_size=(8, 8), \n",
    "                            fusion_output_size=(128, 128), \n",
    "                            model_choice = \"Unet_1\", \n",
    "                            input_choice='RD',\n",
    "                            resnet_saved_model_path=pretrained_model_path,\n",
    "                            dropout_rate=0.5,\n",
    "                            number_of_in_channels=2).to(device)\n",
    "\n",
    "state_dict = torch.load('./artifacts/models/dem2so/model_epoch_900.pth')\n",
    "\n",
    "\n",
    "new_state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "\n",
    "SO_detection_model.load_state_dict(new_state_dict)\n",
    "\n",
    "# model = RGB_DEM_to_SO(resnet_output_size=(8, 8), \n",
    "#                             fusion_output_size=(128, 128), \n",
    "#                             model_choice = \"Unet_1\", \n",
    "#                             resnet_saved_model_path=pretrained_model_path,\n",
    "#                             dropout_rate=0.5).to(device)\n",
    "\n",
    "from torch.optim import Adam\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "\n",
    "state_dict = torch.load('./artifacts/models/model_epoch_100.pth')\n",
    "# state_dict_new = torch.load('./artifacts/new_loss/model_epoch_600.pth')\n",
    "\n",
    "new_state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "classification_model.load_state_dict(new_state_dict)\n",
    "\n",
    "print(len(train_loader))\n",
    "\n",
    "classification_model.eval()\n",
    "SO_detection_model.eval()\n",
    "\n",
    "for i, batch in enumerate(tqdm(train_loader)):\n",
    "# for i, batch in enumerate(train_loader):\n",
    "    \n",
    "    dem = batch['DEM'].to(device)\n",
    "    so = batch['SO'].to(device)\n",
    "    rgbs = [batch['RGB'][k].to(device) for k in range(6)]\n",
    "\n",
    "    permute_rgbs = [torch.permute(image,(0,2,1,3)) for image in rgbs]\n",
    "\n",
    "    \n",
    "    classification_output = classification_model(permute_rgbs)\n",
    "    SO_detection_output = SO_detection_model(dem, permute_rgbs)\n",
    "    \n",
    "    \n",
    "    pred = F.softmax(SO_detection_output, dim=1)              \n",
    "    pred = torch.argmax(pred, dim=1).squeeze(1)\n",
    "    \n",
    "    \n",
    "    final_risk_map = pred * classification_output\n",
    "#     print(output)\n",
    "    \n",
    "#     break\n",
    "    \n",
    "    profile = {\n",
    "    'transform': batch['SO_transform'],  # Example values: (west, north, xsize, ysize)\n",
    "    'crs': 'EPSG:4326',  # Standard WGS84 CRS\n",
    "    }\n",
    "\n",
    "#     # Path to save the GeoTIFF file\n",
    "    root_path = f'/home/macula/SMATousi/Gullies/ground_truth/organized_data/tiled_HUCs/HUC_070801030408/so+classification_results'\n",
    "    os.makedirs(root_path, exist_ok=True)\n",
    "    output_file_path = f'{root_path}/{i}.tif'\n",
    "    \n",
    "\n",
    "    # Call the function to save the file\n",
    "#     print(output.float().cpu().detach().numpy().squeeze())\n",
    "    save_as_geotiff(final_risk_map.float().cpu().detach().numpy().squeeze(), \n",
    "                    profile, \n",
    "                    output_file_path, \n",
    "                    single_value=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     break\n",
    "input_folder = root_path  # Update this path to your folder path\n",
    "output_filepath = f'/home/macula/SMATousi/Gullies/ground_truth/organized_data/tiled_HUCs/HUC_070801030408/merged_so+classification_results.tif'  # Set your output file path\n",
    "\n",
    "merge_tiffs(input_folder, output_filepath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2856aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = f'/home/macula/SMATousi/Gullies/SO_Paper/data/{field_name}/so_2_results/'  # Update this path to your folder path\n",
    "output_filepath = f'/home/macula/SMATousi/Gullies/SO_Paper/data/{field_name}/so_2_results.tif'  # Set your output file path\n",
    "\n",
    "merge_tiffs(input_folder, output_filepath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91c13ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fusion_net.conv.weight\n",
      "fusion_net.conv.bias\n",
      "unet.inc.double_conv.0.weight\n",
      "unet.inc.double_conv.1.weight\n",
      "unet.inc.double_conv.1.bias\n",
      "unet.inc.double_conv.3.weight\n",
      "unet.inc.double_conv.4.weight\n",
      "unet.inc.double_conv.4.bias\n",
      "unet.down1.double_conv.0.weight\n",
      "unet.down1.double_conv.1.weight\n",
      "unet.down1.double_conv.1.bias\n",
      "unet.down1.double_conv.3.weight\n",
      "unet.down1.double_conv.4.weight\n",
      "unet.down1.double_conv.4.bias\n",
      "unet.down2.double_conv.0.weight\n",
      "unet.down2.double_conv.1.weight\n",
      "unet.down2.double_conv.1.bias\n",
      "unet.down2.double_conv.3.weight\n",
      "unet.down2.double_conv.4.weight\n",
      "unet.down2.double_conv.4.bias\n",
      "unet.down3.double_conv.0.weight\n",
      "unet.down3.double_conv.1.weight\n",
      "unet.down3.double_conv.1.bias\n",
      "unet.down3.double_conv.3.weight\n",
      "unet.down3.double_conv.4.weight\n",
      "unet.down3.double_conv.4.bias\n",
      "unet.down4.double_conv.0.weight\n",
      "unet.down4.double_conv.1.weight\n",
      "unet.down4.double_conv.1.bias\n",
      "unet.down4.double_conv.3.weight\n",
      "unet.down4.double_conv.4.weight\n",
      "unet.down4.double_conv.4.bias\n",
      "unet.up1.double_conv.0.weight\n",
      "unet.up1.double_conv.1.weight\n",
      "unet.up1.double_conv.1.bias\n",
      "unet.up1.double_conv.3.weight\n",
      "unet.up1.double_conv.4.weight\n",
      "unet.up1.double_conv.4.bias\n",
      "unet.up2.double_conv.0.weight\n",
      "unet.up2.double_conv.1.weight\n",
      "unet.up2.double_conv.1.bias\n",
      "unet.up2.double_conv.3.weight\n",
      "unet.up2.double_conv.4.weight\n",
      "unet.up2.double_conv.4.bias\n",
      "unet.up3.double_conv.0.weight\n",
      "unet.up3.double_conv.1.weight\n",
      "unet.up3.double_conv.1.bias\n",
      "unet.up3.double_conv.3.weight\n",
      "unet.up3.double_conv.4.weight\n",
      "unet.up3.double_conv.4.bias\n",
      "unet.up4.double_conv.0.weight\n",
      "unet.up4.double_conv.1.weight\n",
      "unet.up4.double_conv.1.bias\n",
      "unet.up4.double_conv.3.weight\n",
      "unet.up4.double_conv.4.weight\n",
      "unet.up4.double_conv.4.bias\n",
      "unet.outc.weight\n",
      "unet.outc.bias\n",
      "unet_light.inc.double_conv.0.weight\n",
      "unet_light.inc.double_conv.1.weight\n",
      "unet_light.inc.double_conv.1.bias\n",
      "unet_light.inc.double_conv.3.weight\n",
      "unet_light.inc.double_conv.4.weight\n",
      "unet_light.inc.double_conv.4.bias\n",
      "unet_light.down1.double_conv.0.weight\n",
      "unet_light.down1.double_conv.1.weight\n",
      "unet_light.down1.double_conv.1.bias\n",
      "unet_light.down1.double_conv.3.weight\n",
      "unet_light.down1.double_conv.4.weight\n",
      "unet_light.down1.double_conv.4.bias\n",
      "unet_light.down2.double_conv.0.weight\n",
      "unet_light.down2.double_conv.1.weight\n",
      "unet_light.down2.double_conv.1.bias\n",
      "unet_light.down2.double_conv.3.weight\n",
      "unet_light.down2.double_conv.4.weight\n",
      "unet_light.down2.double_conv.4.bias\n",
      "unet_light.down3.double_conv.0.weight\n",
      "unet_light.down3.double_conv.1.weight\n",
      "unet_light.down3.double_conv.1.bias\n",
      "unet_light.down3.double_conv.3.weight\n",
      "unet_light.down3.double_conv.4.weight\n",
      "unet_light.down3.double_conv.4.bias\n",
      "unet_light.down4.double_conv.0.weight\n",
      "unet_light.down4.double_conv.1.weight\n",
      "unet_light.down4.double_conv.1.bias\n",
      "unet_light.down4.double_conv.3.weight\n",
      "unet_light.down4.double_conv.4.weight\n",
      "unet_light.down4.double_conv.4.bias\n",
      "unet_light.up1.double_conv.0.weight\n",
      "unet_light.up1.double_conv.1.weight\n",
      "unet_light.up1.double_conv.1.bias\n",
      "unet_light.up1.double_conv.3.weight\n",
      "unet_light.up1.double_conv.4.weight\n",
      "unet_light.up1.double_conv.4.bias\n",
      "unet_light.up2.double_conv.0.weight\n",
      "unet_light.up2.double_conv.1.weight\n",
      "unet_light.up2.double_conv.1.bias\n",
      "unet_light.up2.double_conv.3.weight\n",
      "unet_light.up2.double_conv.4.weight\n",
      "unet_light.up2.double_conv.4.bias\n",
      "unet_light.up3.double_conv.0.weight\n",
      "unet_light.up3.double_conv.1.weight\n",
      "unet_light.up3.double_conv.1.bias\n",
      "unet_light.up3.double_conv.3.weight\n",
      "unet_light.up3.double_conv.4.weight\n",
      "unet_light.up3.double_conv.4.bias\n",
      "unet_light.up4.double_conv.0.weight\n",
      "unet_light.up4.double_conv.1.weight\n",
      "unet_light.up4.double_conv.1.bias\n",
      "unet_light.up4.double_conv.3.weight\n",
      "unet_light.up4.double_conv.4.weight\n",
      "unet_light.up4.double_conv.4.bias\n",
      "unet_light.outc.weight\n",
      "unet_light.outc.bias\n",
      "onet.inc.double_conv.0.weight\n",
      "onet.inc.double_conv.1.weight\n",
      "onet.inc.double_conv.1.bias\n",
      "onet.inc.double_conv.3.weight\n",
      "onet.inc.double_conv.4.weight\n",
      "onet.inc.double_conv.4.bias\n",
      "onet.top_up_1.double_conv.0.weight\n",
      "onet.top_up_1.double_conv.1.weight\n",
      "onet.top_up_1.double_conv.1.bias\n",
      "onet.top_up_1.double_conv.3.weight\n",
      "onet.top_up_1.double_conv.4.weight\n",
      "onet.top_up_1.double_conv.4.bias\n",
      "onet.top_up_2.double_conv.0.weight\n",
      "onet.top_up_2.double_conv.1.weight\n",
      "onet.top_up_2.double_conv.1.bias\n",
      "onet.top_up_2.double_conv.3.weight\n",
      "onet.top_up_2.double_conv.4.weight\n",
      "onet.top_up_2.double_conv.4.bias\n",
      "onet.top_up_3.double_conv.0.weight\n",
      "onet.top_up_3.double_conv.1.weight\n",
      "onet.top_up_3.double_conv.1.bias\n",
      "onet.top_up_3.double_conv.3.weight\n",
      "onet.top_up_3.double_conv.4.weight\n",
      "onet.top_up_3.double_conv.4.bias\n",
      "onet.top_up_4.double_conv.0.weight\n",
      "onet.top_up_4.double_conv.1.weight\n",
      "onet.top_up_4.double_conv.1.bias\n",
      "onet.top_up_4.double_conv.3.weight\n",
      "onet.top_up_4.double_conv.4.weight\n",
      "onet.top_up_4.double_conv.4.bias\n",
      "onet.top_down_1.double_conv.0.weight\n",
      "onet.top_down_1.double_conv.1.weight\n",
      "onet.top_down_1.double_conv.1.bias\n",
      "onet.top_down_1.double_conv.3.weight\n",
      "onet.top_down_1.double_conv.4.weight\n",
      "onet.top_down_1.double_conv.4.bias\n",
      "onet.top_down_2.double_conv.0.weight\n",
      "onet.top_down_2.double_conv.1.weight\n",
      "onet.top_down_2.double_conv.1.bias\n",
      "onet.top_down_2.double_conv.3.weight\n",
      "onet.top_down_2.double_conv.4.weight\n",
      "onet.top_down_2.double_conv.4.bias\n",
      "onet.top_down_3.double_conv.0.weight\n",
      "onet.top_down_3.double_conv.1.weight\n",
      "onet.top_down_3.double_conv.1.bias\n",
      "onet.top_down_3.double_conv.3.weight\n",
      "onet.top_down_3.double_conv.4.weight\n",
      "onet.top_down_3.double_conv.4.bias\n",
      "onet.bot_up_1.double_conv.0.weight\n",
      "onet.bot_up_1.double_conv.1.weight\n",
      "onet.bot_up_1.double_conv.1.bias\n",
      "onet.bot_up_1.double_conv.3.weight\n",
      "onet.bot_up_1.double_conv.4.weight\n",
      "onet.bot_up_1.double_conv.4.bias\n",
      "onet.bot_up_2.double_conv.0.weight\n",
      "onet.bot_up_2.double_conv.1.weight\n",
      "onet.bot_up_2.double_conv.1.bias\n",
      "onet.bot_up_2.double_conv.3.weight\n",
      "onet.bot_up_2.double_conv.4.weight\n",
      "onet.bot_up_2.double_conv.4.bias\n",
      "onet.bot_up_3.double_conv.0.weight\n",
      "onet.bot_up_3.double_conv.1.weight\n",
      "onet.bot_up_3.double_conv.1.bias\n",
      "onet.bot_up_3.double_conv.3.weight\n",
      "onet.bot_up_3.double_conv.4.weight\n",
      "onet.bot_up_3.double_conv.4.bias\n",
      "onet.bot_down_1.double_conv.0.weight\n",
      "onet.bot_down_1.double_conv.1.weight\n",
      "onet.bot_down_1.double_conv.1.bias\n",
      "onet.bot_down_1.double_conv.3.weight\n",
      "onet.bot_down_1.double_conv.4.weight\n",
      "onet.bot_down_1.double_conv.4.bias\n",
      "onet.bot_down_2.double_conv.0.weight\n",
      "onet.bot_down_2.double_conv.1.weight\n",
      "onet.bot_down_2.double_conv.1.bias\n",
      "onet.bot_down_2.double_conv.3.weight\n",
      "onet.bot_down_2.double_conv.4.weight\n",
      "onet.bot_down_2.double_conv.4.bias\n",
      "onet.bot_down_3.double_conv.0.weight\n",
      "onet.bot_down_3.double_conv.1.weight\n",
      "onet.bot_down_3.double_conv.1.bias\n",
      "onet.bot_down_3.double_conv.3.weight\n",
      "onet.bot_down_3.double_conv.4.weight\n",
      "onet.bot_down_3.double_conv.4.bias\n",
      "onet.bot_down_4.double_conv.0.weight\n",
      "onet.bot_down_4.double_conv.1.weight\n",
      "onet.bot_down_4.double_conv.1.bias\n",
      "onet.bot_down_4.double_conv.3.weight\n",
      "onet.bot_down_4.double_conv.4.weight\n",
      "onet.bot_down_4.double_conv.4.bias\n",
      "onet.out_mid.double_conv.0.weight\n",
      "onet.out_mid.double_conv.1.weight\n",
      "onet.out_mid.double_conv.1.bias\n",
      "onet.out_mid.double_conv.3.weight\n",
      "onet.out_mid.double_conv.4.weight\n",
      "onet.out_mid.double_conv.4.bias\n",
      "onet.outc.weight\n",
      "onet.outc.bias\n",
      "onet.final.weight\n",
      "onet.final.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in SO_detection_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a16817",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
