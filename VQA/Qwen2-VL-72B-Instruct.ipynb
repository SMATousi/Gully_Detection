{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77c4fd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42f734af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n",
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████| 38/38 [08:01<00:00, 12.67s/it]\n"
     ]
    }
   ],
   "source": [
    "# model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "#     \"Qwen/Qwen2-VL-72B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\").eval()\n",
    "\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-72B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53e7bb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-72B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9850b176",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/root/home/data/pos_collage_903.jpg'\n",
    "messages = [\n",
    "    {\"role\": \"user\",\"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},],}]\n",
    "\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor( text=[text], images=image_inputs, padding=True, return_tensors=\"pt\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa984e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor( text=[text], images=image_inputs, padding=True, return_tensors=\"pt\",)\n",
    "inputs = inputs.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b5bbcd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(**inputs, max_new_tokens=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "77de363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "438b4baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The image depicts a serene beach scene at sunset. A person is sitting on the sand, facing the ocean, with their back to the camera. They are wearing a plaid shirt and shorts. Next to them, a large dog, possibly a Labrador Retriever, is sitting upright, facing the person. The dog is wearing a harness. The sun is setting in the background, casting a warm glow over the entire scene, creating a peaceful and tranquil atmosphere. The waves gently lap at the shore, adding to the calm ambiance.']\n"
     ]
    }
   ],
   "source": [
    "output_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a5927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\"The image shows a close-up of a person's hand holding a smartphone. The screen of the smartphone displays a map application with various locations marked and routes indicated. The map appears to be centered around a specific area, possibly for navigation or location tracking purposes. The background is blurred, focusing attention on the smartphone screen.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "896240f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_text==[\"The image shows a close-up of a person's hand holding a smartphone. The screen of the smartphone displays a map application with various locations marked and routes indicated. The map appears to be centered around a specific area, possibly for navigation or location tracking purposes. The background is blurred, focusing attention on the smartphone screen.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e703c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
