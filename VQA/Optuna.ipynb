{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4874abb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import ollama \n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import signal\n",
    "import argparse\n",
    "import wandb\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score,confusion_matrix\n",
    "from pathlib import Path\n",
    "\n",
    "import optuna\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5627210e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac0183d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = [\n",
    "    'notebook',  \n",
    "    '--modelname', 'llama3.2-vision:90b',\n",
    "    '--data', '/root/home/data',\n",
    "    '--data_path_files','/mnt/Gbenga_Enemy/ramy/WACV-2025-Workshop-ViGIR',\n",
    "    '--results_dir', '/mnt/Gbenga_Enemy/ramy/results',\n",
    "    '--timeout', '20',\n",
    "    '--model_unloading',\n",
    "    '--valid_responses' ,'/mnt/Gbenga_Enemy/ramy/results/valid_llama3_90b.json',\n",
    "    '--test_responses' ,'/mnt/Gbenga_Enemy/ramy/results/test_llama3_90b.json'\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9444b7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"A script to run V-LLMs on different image classification datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "327cdc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.add_argument(\"--modelname\", type=str, required=True, help=\"The name of the V-LLM model\")\n",
    "parser.add_argument(\"--data\", type=str, required=True, help=\"Path to the data\")\n",
    "parser.add_argument(\"--data_path_files\", type=str, required=True, help=\"Path to the image data dir\")\n",
    "parser.add_argument(\"--results_dir\", type=str, required=True, help=\"Folder name to save results\")\n",
    "parser.add_argument(\"--timeout\", type=int, default=40, help=\"time out duration to skip one sample\")\n",
    "parser.add_argument(\"--model_unloading\", action=\"store_true\", help=\"Enables unloading mode. Every 100 sampels it unloades the model from the GPU to avoid carshing.\")\n",
    "parser.add_argument(\"--valid_responses\",type=str, required=True)\n",
    "parser.add_argument(\"--test_responses\",type=str, required=True)\n",
    "\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38bc8a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mramytrm\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/Gbenga_Enemy/ramy/WACV-2025-Workshop-ViGIR/vqa/wandb/run-20241115_023607-zvb984q8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ramytrm/WACV-2025-QuestionFiltering/runs/zvb984q8' target=\"_blank\">run_test_llama3.2-vision:90bQuestionFiltering</a></strong> to <a href='https://wandb.ai/ramytrm/WACV-2025-QuestionFiltering' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ramytrm/WACV-2025-QuestionFiltering' target=\"_blank\">https://wandb.ai/ramytrm/WACV-2025-QuestionFiltering</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ramytrm/WACV-2025-QuestionFiltering/runs/zvb984q8' target=\"_blank\">https://wandb.ai/ramytrm/WACV-2025-QuestionFiltering/runs/zvb984q8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    entity=\"ramytrm\",\n",
    "    project=f\"WACV-2025-QuestionFiltering\",\n",
    "    name=\"run_test_\" +args.modelname+\"QuestionFiltering\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c043214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(image_path):\n",
    "    img = mpimg.imread(image_path)\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b88f5e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/Gbenga_Enemy/ramy/results/Best_Q_Best_Dict_regularized.json\n"
     ]
    }
   ],
   "source": [
    "best_dict_path=os.path.join(args.results_dir,'Best_Q_Best_Dict_regularized.json')\n",
    "print(best_dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e06c9f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = args.modelname #'llama3.2-vision:90b'\n",
    "ollama.pull(model_name)\n",
    "\n",
    "timeout_duration = args.timeout\n",
    "\n",
    "options= {  # new\n",
    "            \"seed\": 123,\n",
    "            \"temperature\": 0,\n",
    "            \"num_ctx\": 2048, # must be set, otherwise slightly random output\n",
    "        }\n",
    "\n",
    "model_labels = {}\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f580822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(args.valid_responses)\n",
    "with open(file_path, 'r') as file:\n",
    "    valid_responses = json.load(file)\n",
    "    \n",
    "    \n",
    "file_path = os.path.join(args.test_responses)\n",
    "with open(file_path, 'r') as file:\n",
    "    test_responses = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "697fa34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reformated = {}\n",
    "\n",
    "for key, item in valid_responses.items():\n",
    "    q_and_a = []\n",
    "    for i in range(len(item)):\n",
    "        tmp = item[i][1:]   \n",
    "        q_and_a.append(tmp)\n",
    "\n",
    "    data_reformated[key] = q_and_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7e5975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reformated_test = {}\n",
    "\n",
    "for key, item in test_responses.items():\n",
    "    q_and_a = []\n",
    "    for i in range(len(item)):\n",
    "        tmp = item[i][1:]   \n",
    "        q_and_a.append(tmp)\n",
    "\n",
    "    data_reformated_test[key] = q_and_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5168258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "621"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_reformated)+len(data_reformated_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "099b72db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "binary_vars=[\n",
    "            0,\n",
    "            0,\n",
    "            1,\n",
    "            0,\n",
    "            0,\n",
    "            1,\n",
    "            1,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            1,\n",
    "            0,\n",
    "            0,\n",
    "            0\n",
    "        ]\n",
    "print(sum(binary_vars))\n",
    "\n",
    "for index, (key, item) in enumerate(data_reformated.items()):\n",
    "\n",
    "    formatted_qas = []\n",
    "    for i, qa in enumerate(item):\n",
    "        if binary_vars[i] == 0:\n",
    "            continue\n",
    "        formatted_qa = f\"Q: {qa[0]}\\nA: {qa[1]}\"\n",
    "        formatted_qas.append(formatted_qa)\n",
    "\n",
    "    prompt = \"\\n\".join(formatted_qas)\n",
    "\n",
    "    question = \"Based on the following questions and their answers, determine if there is evidence of an ephemeral gully in the observed area. Carefully analyze all the questions and the responses to assess.\\n\\n\"\n",
    "    question += prompt\n",
    "    question += \"\\n\\nAfter considering these responses, provide a clear conclusion: Is there evidence of an ephemeral gully? Answer with yes or no only.\"\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb700d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c99dfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the following questions and their answers, determine if there is evidence of an ephemeral gully in the observed area. Carefully analyze all the questions and the responses to assess.\n",
      "\n",
      "Q: Given these six images of the exact same area and collected over a period of 10 years, do you see a low point in the terrain? Answer with yes or no only!\n",
      "A: No.\n",
      "Q: Given these six images of the exact same area and collected over a period of 10 years, are there indications of nearby human activity, such as tillage or machinery tracks? Answer with yes or no only!\n",
      "A: No.\n",
      "\n",
      "After considering these responses, provide a clear conclusion: Is there evidence of an ephemeral gully? Answer with yes or no only.\n"
     ]
    }
   ],
   "source": [
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f285f687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_first_yes_no(text):\n",
    "    # Use regular expressions to find \"Yes\" or \"No\" (case-insensitive)\n",
    "    match = re.search(r'\\b(Yes|No)\\b', text, re.IGNORECASE)\n",
    "    if match:\n",
    "        # Return the first match in its original case\n",
    "        return match.group(0)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f848b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(data_reform, responses, binary_vars, model_name, options,trial,VALID=0):\n",
    "    ground_truth_test = []\n",
    "    model_response_test = []\n",
    "\n",
    "    for index, (key, item) in enumerate(data_reform.items()):\n",
    "\n",
    "        formatted_qas = []\n",
    "        for i, qa in enumerate(item):\n",
    "            if binary_vars[i] == 0:\n",
    "                continue\n",
    "            formatted_qa = f\"Q: {qa[0]}\\nA: {qa[1]}\"\n",
    "            formatted_qas.append(formatted_qa)\n",
    "\n",
    "        prompt = \"\\n\".join(formatted_qas)\n",
    "\n",
    "        question = \"Based on the following questions and their answers, determine if there is evidence of an ephemeral gully in the observed area. Carefully analyze all the questions and the responses to assess.\\n\\n\"\n",
    "        question += prompt\n",
    "        question += \"\\n\\nAfter considering these responses, provide a clear conclusion: Is there evidence of an ephemeral gully? Answer with yes or no only.\"\n",
    "#         question += \"\\n\\nIs there evidence of an ephemeral gully? Answer with yes or no only.\"\n",
    "\n",
    "        response = ollama.generate(model=model_name, prompt=question, options=options)\n",
    "        text=extract_first_yes_no(response['response'])\n",
    "        \n",
    "        \n",
    "        if text == 'Yes':\n",
    "            model_response_test.append(1)\n",
    "        else:\n",
    "            model_response_test.append(0)\n",
    "            \n",
    "        if responses[key][0][0]['label'] == '4':\n",
    "            ground_truth_test.append(1)\n",
    "        else:\n",
    "            ground_truth_test.append(0)\n",
    "            \n",
    "            \n",
    "        if VALID==1 and index%50==0:\n",
    "        \n",
    "            macro_f1 = f1_score(ground_truth_test, model_response_test, average='macro')\n",
    "            macro_f1-=sum(binary_vars)/600\n",
    "            trial.report(macro_f1, step=index)\n",
    "            \n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "    return ground_truth_test, model_response_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9263110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b725e53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_best_dict(best_dict,file_path):\n",
    "\n",
    "    wandb.log(best_dict)\n",
    "    \n",
    "    table = wandb.Table(data=[list(best_dict.values())], columns=list(best_dict.keys()))\n",
    "    wandb.log({\"best_dict_table\": table})\n",
    "    \n",
    "    if os.path.isfile(file_path):\n",
    "        artifact = wandb.Artifact(\"best_dict\", type=\"dictionary\")\n",
    "        artifact.add_file(file_path)\n",
    "        wandb.log_artifact(artifact)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"The file {file_path} was not created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bbe50a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    global best_dict, trail_no\n",
    "    \n",
    "    trail_no+=1\n",
    "    \n",
    "    print(trail_no,best_dict['best_macro'])\n",
    "    \n",
    "    binary_vars = [trial.suggest_int(f\"x{i}\", 0, 1) for i in range(15)]\n",
    "    \n",
    "\n",
    "    if sum(binary_vars)<2:\n",
    "        return 0\n",
    "    \n",
    "    ground_truth, model_response = evaluate_model(data_reformated, \n",
    "                                                  valid_responses, \n",
    "                                                  binary_vars, \n",
    "                                                  model_name, \n",
    "                                                  options,trial, \n",
    "                                                  VALID=1)\n",
    "        \n",
    "    \n",
    "    macro_f1 = f1_score(ground_truth, model_response, average='macro')\n",
    "    \n",
    "    macro_f1-=sum(binary_vars)/600\n",
    "    \n",
    "    \n",
    "    if macro_f1>best_dict['best_macro']:\n",
    "        \n",
    "        \n",
    "        best_dict['all_best']=[]\n",
    "        best_dict['all_best'].append(binary_vars)\n",
    "        best_dict['best_macro']=macro_f1\n",
    "        best_dict['best_questions']=binary_vars\n",
    "        \n",
    "        \n",
    "        ground_truth_test, model_response_test = evaluate_model(\n",
    "        data_reformated_test, test_responses, binary_vars, model_name, options, trial\n",
    "        )\n",
    "\n",
    "\n",
    "        macro_f1_test = f1_score(ground_truth_test, model_response_test, average='macro')\n",
    "        best_dict['best_test']=macro_f1_test\n",
    "        \n",
    "        print('Got new best', best_dict['best_macro'],best_dict['best_test'])\n",
    "        \n",
    "        with open(best_dict_path, 'w') as fp:\n",
    "            json.dump(best_dict, fp, indent=4)\n",
    "            \n",
    "        log_best_dict(best_dict,best_dict_path)\n",
    "        \n",
    "    return macro_f1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaecf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 20:01:49,653] A new study created in memory with name: no-name-bab9558c-98d3-40dd-94d6-fbd8e484151c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0\n",
      "Got new best 0.6431138975966562 0.6325858066655613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 20:34:06,476] Trial 0 finished with value: 0.6431138975966562 and parameters: {'x0': 1, 'x1': 1, 'x2': 1, 'x3': 0, 'x4': 1, 'x5': 1, 'x6': 1, 'x7': 1, 'x8': 1, 'x9': 0, 'x10': 0, 'x11': 1, 'x12': 1, 'x13': 1, 'x14': 1}. Best is trial 0 with value: 0.6431138975966562.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.6431138975966562\n",
      "Got new best 0.6751854642098544 0.6359181559181559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 21:10:48,688] Trial 1 finished with value: 0.6751854642098544 and parameters: {'x0': 1, 'x1': 1, 'x2': 0, 'x3': 1, 'x4': 0, 'x5': 1, 'x6': 1, 'x7': 1, 'x8': 1, 'x9': 1, 'x10': 0, 'x11': 0, 'x12': 1, 'x13': 0, 'x14': 1}. Best is trial 1 with value: 0.6751854642098544.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.6751854642098544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 21:12:40,551] Trial 2 finished with value: 0.32536098923015555 and parameters: {'x0': 1, 'x1': 0, 'x2': 0, 'x3': 1, 'x4': 0, 'x5': 0, 'x6': 0, 'x7': 1, 'x8': 1, 'x9': 0, 'x10': 0, 'x11': 1, 'x12': 0, 'x13': 1, 'x14': 0}. Best is trial 1 with value: 0.6751854642098544.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 0.6751854642098544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 21:38:01,090] Trial 3 finished with value: 0.6751854642098544 and parameters: {'x0': 0, 'x1': 1, 'x2': 1, 'x3': 1, 'x4': 0, 'x5': 1, 'x6': 1, 'x7': 1, 'x8': 0, 'x9': 1, 'x10': 0, 'x11': 0, 'x12': 1, 'x13': 1, 'x14': 1}. Best is trial 1 with value: 0.6751854642098544.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 0.6751854642098544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 21:53:28,033] Trial 4 finished with value: 0.6663716608472876 and parameters: {'x0': 0, 'x1': 1, 'x2': 1, 'x3': 0, 'x4': 1, 'x5': 1, 'x6': 1, 'x7': 1, 'x8': 1, 'x9': 1, 'x10': 1, 'x11': 1, 'x12': 0, 'x13': 0, 'x14': 1}. Best is trial 1 with value: 0.6751854642098544.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 0.6751854642098544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 22:09:15,583] Trial 5 finished with value: 0.6751854642098544 and parameters: {'x0': 1, 'x1': 0, 'x2': 0, 'x3': 1, 'x4': 0, 'x5': 1, 'x6': 1, 'x7': 1, 'x8': 1, 'x9': 1, 'x10': 0, 'x11': 0, 'x12': 1, 'x13': 1, 'x14': 1}. Best is trial 1 with value: 0.6751854642098544.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 0.6751854642098544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 22:09:16,382] Trial 6 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 0.6751854642098544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 22:09:17,124] Trial 7 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 0.6751854642098544\n",
      "Got new best 0.6785187975431878 0.6359181559181559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 22:37:14,887] Trial 8 finished with value: 0.6785187975431878 and parameters: {'x0': 1, 'x1': 0, 'x2': 1, 'x3': 1, 'x4': 0, 'x5': 0, 'x6': 1, 'x7': 0, 'x8': 0, 'x9': 1, 'x10': 0, 'x11': 1, 'x12': 1, 'x13': 1, 'x14': 0}. Best is trial 8 with value: 0.6785187975431878.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0.6785187975431878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 22:37:15,692] Trial 9 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 0.6785187975431878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 22:37:16,383] Trial 10 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 0.6785187975431878\n",
      "Got new best 0.6801854642098544 0.6359181559181559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 23:01:30,525] Trial 11 finished with value: 0.6801854642098544 and parameters: {'x0': 1, 'x1': 0, 'x2': 0, 'x3': 1, 'x4': 0, 'x5': 0, 'x6': 1, 'x7': 0, 'x8': 0, 'x9': 1, 'x10': 1, 'x11': 0, 'x12': 1, 'x13': 0, 'x14': 1}. Best is trial 11 with value: 0.6801854642098544.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 0.6801854642098544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 23:17:31,780] Trial 12 finished with value: 0.6785187975431878 and parameters: {'x0': 1, 'x1': 0, 'x2': 1, 'x3': 1, 'x4': 0, 'x5': 0, 'x6': 1, 'x7': 0, 'x8': 0, 'x9': 1, 'x10': 1, 'x11': 1, 'x12': 1, 'x13': 0, 'x14': 0}. Best is trial 11 with value: 0.6801854642098544.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 0.6801854642098544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 23:29:56,028] Trial 13 finished with value: 0.6801854642098544 and parameters: {'x0': 1, 'x1': 0, 'x2': 0, 'x3': 1, 'x4': 0, 'x5': 0, 'x6': 1, 'x7': 0, 'x8': 0, 'x9': 1, 'x10': 1, 'x11': 0, 'x12': 1, 'x13': 0, 'x14': 1}. Best is trial 11 with value: 0.6801854642098544.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 0.6801854642098544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 23:42:20,064] Trial 14 finished with value: 0.6801854642098544 and parameters: {'x0': 1, 'x1': 0, 'x2': 0, 'x3': 1, 'x4': 0, 'x5': 0, 'x6': 1, 'x7': 0, 'x8': 0, 'x9': 1, 'x10': 1, 'x11': 0, 'x12': 1, 'x13': 0, 'x14': 1}. Best is trial 11 with value: 0.6801854642098544.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 0.6801854642098544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 23:54:44,206] Trial 15 finished with value: 0.6801854642098544 and parameters: {'x0': 1, 'x1': 0, 'x2': 0, 'x3': 1, 'x4': 0, 'x5': 0, 'x6': 1, 'x7': 0, 'x8': 0, 'x9': 1, 'x10': 1, 'x11': 0, 'x12': 1, 'x13': 0, 'x14': 1}. Best is trial 11 with value: 0.6801854642098544.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 0.6801854642098544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 00:07:08,315] Trial 16 finished with value: 0.6801854642098544 and parameters: {'x0': 1, 'x1': 0, 'x2': 0, 'x3': 1, 'x4': 0, 'x5': 0, 'x6': 1, 'x7': 0, 'x8': 0, 'x9': 1, 'x10': 1, 'x11': 0, 'x12': 1, 'x13': 0, 'x14': 1}. Best is trial 11 with value: 0.6801854642098544.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 0.6801854642098544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 00:07:08,758] Trial 17 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 0.6801854642098544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 00:19:33,198] Trial 18 finished with value: 0.6801854642098544 and parameters: {'x0': 1, 'x1': 0, 'x2': 0, 'x3': 1, 'x4': 0, 'x5': 0, 'x6': 1, 'x7': 0, 'x8': 0, 'x9': 1, 'x10': 1, 'x11': 0, 'x12': 1, 'x13': 0, 'x14': 1}. Best is trial 11 with value: 0.6801854642098544.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 0.6801854642098544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 00:31:57,197] Trial 19 finished with value: 0.6801854642098544 and parameters: {'x0': 1, 'x1': 0, 'x2': 0, 'x3': 1, 'x4': 0, 'x5': 0, 'x6': 1, 'x7': 0, 'x8': 0, 'x9': 1, 'x10': 1, 'x11': 0, 'x12': 1, 'x13': 0, 'x14': 1}. Best is trial 11 with value: 0.6801854642098544.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 0.6801854642098544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 00:31:57,746] Trial 20 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 0.6801854642098544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 00:44:22,107] Trial 21 finished with value: 0.6801854642098544 and parameters: {'x0': 1, 'x1': 0, 'x2': 0, 'x3': 1, 'x4': 0, 'x5': 0, 'x6': 1, 'x7': 0, 'x8': 0, 'x9': 1, 'x10': 1, 'x11': 0, 'x12': 1, 'x13': 0, 'x14': 1}. Best is trial 11 with value: 0.6801854642098544.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 0.6801854642098544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 00:56:46,034] Trial 22 finished with value: 0.6801854642098544 and parameters: {'x0': 1, 'x1': 0, 'x2': 0, 'x3': 1, 'x4': 0, 'x5': 0, 'x6': 1, 'x7': 0, 'x8': 0, 'x9': 1, 'x10': 1, 'x11': 0, 'x12': 1, 'x13': 0, 'x14': 1}. Best is trial 11 with value: 0.6801854642098544.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 0.6801854642098544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 01:09:10,889] Trial 23 finished with value: 0.6801854642098544 and parameters: {'x0': 1, 'x1': 0, 'x2': 0, 'x3': 1, 'x4': 0, 'x5': 0, 'x6': 1, 'x7': 0, 'x8': 0, 'x9': 1, 'x10': 1, 'x11': 0, 'x12': 1, 'x13': 0, 'x14': 1}. Best is trial 11 with value: 0.6801854642098544.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 0.6801854642098544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 01:21:34,996] Trial 24 finished with value: 0.6801854642098544 and parameters: {'x0': 1, 'x1': 0, 'x2': 0, 'x3': 1, 'x4': 0, 'x5': 0, 'x6': 1, 'x7': 0, 'x8': 0, 'x9': 1, 'x10': 1, 'x11': 0, 'x12': 1, 'x13': 0, 'x14': 1}. Best is trial 11 with value: 0.6801854642098544.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 0.6801854642098544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 01:33:58,970] Trial 25 finished with value: 0.6801854642098544 and parameters: {'x0': 1, 'x1': 0, 'x2': 0, 'x3': 1, 'x4': 0, 'x5': 0, 'x6': 1, 'x7': 0, 'x8': 0, 'x9': 1, 'x10': 1, 'x11': 0, 'x12': 1, 'x13': 0, 'x14': 1}. Best is trial 11 with value: 0.6801854642098544.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 0.6801854642098544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 01:46:22,810] Trial 26 finished with value: 0.6801854642098544 and parameters: {'x0': 1, 'x1': 0, 'x2': 0, 'x3': 1, 'x4': 0, 'x5': 0, 'x6': 1, 'x7': 0, 'x8': 0, 'x9': 1, 'x10': 1, 'x11': 0, 'x12': 1, 'x13': 0, 'x14': 1}. Best is trial 11 with value: 0.6801854642098544.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 0.6801854642098544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 01:58:47,168] Trial 27 finished with value: 0.6801854642098544 and parameters: {'x0': 1, 'x1': 0, 'x2': 0, 'x3': 1, 'x4': 0, 'x5': 0, 'x6': 1, 'x7': 0, 'x8': 0, 'x9': 1, 'x10': 1, 'x11': 0, 'x12': 1, 'x13': 0, 'x14': 1}. Best is trial 11 with value: 0.6801854642098544.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 0.6801854642098544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 02:11:11,348] Trial 28 finished with value: 0.6801854642098544 and parameters: {'x0': 1, 'x1': 0, 'x2': 0, 'x3': 1, 'x4': 0, 'x5': 0, 'x6': 1, 'x7': 0, 'x8': 0, 'x9': 1, 'x10': 1, 'x11': 0, 'x12': 1, 'x13': 0, 'x14': 1}. Best is trial 11 with value: 0.6801854642098544.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 0.6801854642098544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 02:25:34,067] Trial 29 finished with value: 0.669604053883923 and parameters: {'x0': 1, 'x1': 0, 'x2': 0, 'x3': 0, 'x4': 1, 'x5': 1, 'x6': 1, 'x7': 0, 'x8': 0, 'x9': 0, 'x10': 1, 'x11': 0, 'x12': 1, 'x13': 0, 'x14': 1}. Best is trial 11 with value: 0.6801854642098544.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 0.6801854642098544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 02:37:58,328] Trial 30 finished with value: 0.6801854642098544 and parameters: {'x0': 1, 'x1': 0, 'x2': 0, 'x3': 1, 'x4': 0, 'x5': 0, 'x6': 1, 'x7': 0, 'x8': 0, 'x9': 1, 'x10': 1, 'x11': 0, 'x12': 1, 'x13': 0, 'x14': 1}. Best is trial 11 with value: 0.6801854642098544.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 0.6801854642098544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 02:50:22,374] Trial 31 finished with value: 0.6801854642098544 and parameters: {'x0': 1, 'x1': 0, 'x2': 0, 'x3': 1, 'x4': 0, 'x5': 0, 'x6': 1, 'x7': 0, 'x8': 0, 'x9': 1, 'x10': 1, 'x11': 0, 'x12': 1, 'x13': 0, 'x14': 1}. Best is trial 11 with value: 0.6801854642098544.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 0.6801854642098544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 03:02:46,271] Trial 32 finished with value: 0.6801854642098544 and parameters: {'x0': 1, 'x1': 0, 'x2': 0, 'x3': 1, 'x4': 0, 'x5': 0, 'x6': 1, 'x7': 0, 'x8': 0, 'x9': 1, 'x10': 1, 'x11': 0, 'x12': 1, 'x13': 0, 'x14': 1}. Best is trial 11 with value: 0.6801854642098544.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 0.6801854642098544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 03:15:10,255] Trial 33 finished with value: 0.6801854642098544 and parameters: {'x0': 1, 'x1': 0, 'x2': 0, 'x3': 1, 'x4': 0, 'x5': 0, 'x6': 1, 'x7': 0, 'x8': 0, 'x9': 1, 'x10': 1, 'x11': 0, 'x12': 1, 'x13': 0, 'x14': 1}. Best is trial 11 with value: 0.6801854642098544.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 0.6801854642098544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 03:15:14,424] Trial 34 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 0.6801854642098544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 03:15:26,150] Trial 35 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 0.6801854642098544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 03:15:32,569] Trial 36 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 0.6801854642098544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 03:27:12,403] Trial 37 finished with value: 0.6801854642098544 and parameters: {'x0': 0, 'x1': 0, 'x2': 0, 'x3': 1, 'x4': 0, 'x5': 0, 'x6': 1, 'x7': 0, 'x8': 1, 'x9': 1, 'x10': 1, 'x11': 1, 'x12': 0, 'x13': 0, 'x14': 1}. Best is trial 11 with value: 0.6801854642098544.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 0.6801854642098544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 03:27:18,420] Trial 38 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 0.6801854642098544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 03:27:19,352] Trial 39 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 0.6801854642098544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 03:27:25,674] Trial 40 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 0.6801854642098544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 03:41:13,859] Trial 41 finished with value: 0.6801854642098544 and parameters: {'x0': 1, 'x1': 0, 'x2': 0, 'x3': 1, 'x4': 0, 'x5': 0, 'x6': 1, 'x7': 0, 'x8': 0, 'x9': 1, 'x10': 1, 'x11': 0, 'x12': 1, 'x13': 0, 'x14': 1}. Best is trial 11 with value: 0.6801854642098544.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 0.6801854642098544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 03:55:04,736] Trial 42 finished with value: 0.6801854642098544 and parameters: {'x0': 1, 'x1': 0, 'x2': 0, 'x3': 1, 'x4': 0, 'x5': 0, 'x6': 1, 'x7': 0, 'x8': 0, 'x9': 1, 'x10': 1, 'x11': 0, 'x12': 1, 'x13': 0, 'x14': 1}. Best is trial 11 with value: 0.6801854642098544.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 0.6801854642098544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 04:08:54,851] Trial 43 finished with value: 0.6801854642098544 and parameters: {'x0': 1, 'x1': 0, 'x2': 0, 'x3': 1, 'x4': 0, 'x5': 0, 'x6': 1, 'x7': 0, 'x8': 0, 'x9': 1, 'x10': 1, 'x11': 0, 'x12': 1, 'x13': 0, 'x14': 1}. Best is trial 11 with value: 0.6801854642098544.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 0.6801854642098544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 04:22:45,015] Trial 44 finished with value: 0.6801854642098544 and parameters: {'x0': 1, 'x1': 0, 'x2': 0, 'x3': 1, 'x4': 0, 'x5': 0, 'x6': 1, 'x7': 0, 'x8': 0, 'x9': 1, 'x10': 1, 'x11': 0, 'x12': 1, 'x13': 0, 'x14': 1}. Best is trial 11 with value: 0.6801854642098544.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 0.6801854642098544\n",
      "Got new best 0.6835187975431878 0.6359181559181559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 04:26:35,588] Trial 45 finished with value: 0.6835187975431878 and parameters: {'x0': 1, 'x1': 0, 'x2': 0, 'x3': 1, 'x4': 0, 'x5': 0, 'x6': 1, 'x7': 0, 'x8': 0, 'x9': 1, 'x10': 0, 'x11': 0, 'x12': 1, 'x13': 0, 'x14': 0}. Best is trial 45 with value: 0.6835187975431878.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 0.6835187975431878\n"
     ]
    }
   ],
   "source": [
    "trail_no=0\n",
    "best_dict={}\n",
    "\n",
    "best_dict['best_macro']=0\n",
    "best_dict['best_questions']=[]\n",
    "best_dict['all_best']=[]\n",
    "\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner())\n",
    "study.optimize(objective, n_trials=1000)\n",
    "\n",
    "\n",
    "print(\"Best objective value:\", study.best_value)\n",
    "print(\"Best parameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "67593740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6445"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tes=[1]*15\n",
    "\n",
    "0.6945 -sum(tes)/300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da64e66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(tes))\n",
    "sum(binary_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ba7b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "tes[1:12]=[0]*11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c157d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef04ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, (key, item) in enumerate(data_reformated.items()):\n",
    "    formatted_qas = []\n",
    "    for i, qa in enumerate(item):\n",
    "        if tes[i]==0:\n",
    "            continue\n",
    "        formatted_qa = f\"Q: {qa[0]}\\nA: {qa[1]}\"\n",
    "        formatted_qas.append(formatted_qa)\n",
    "        \n",
    "    prompt = \"\\n\".join(formatted_qas)\n",
    "    print(prompt)\n",
    "    print('________________________________')\n",
    "    if index ==2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eab91d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reformated[str(356)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa2e5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab48b536",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0178209",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_vars=[1]*15\n",
    "\n",
    "ground_truth, model_response = evaluate_model(data_reformated_test, \n",
    "                                              test_responses, \n",
    "                                              binary_vars, \n",
    "                                              model_name, \n",
    "                                              options, 12\n",
    "                                            )\n",
    "\n",
    "\n",
    "macro_f1 = f1_score(ground_truth, model_response, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd6e857",
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1362cba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96dc43f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 5, 6, 11]\n"
     ]
    }
   ],
   "source": [
    "indices = [index for index, value in enumerate(binary_vars) if value == 1]\n",
    "\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ced74c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_test, model_response_test = evaluate_model(\n",
    "        data_reformated_test, test_responses, binary_vars, model_name, options, 12\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ad8df30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives (TP): 110\n",
      "False Positives (FP): 44\n",
      "False Negatives (FN): 67\n",
      "True Negatives (TN): 90\n",
      "Precision: 0.7142857142857143\n",
      "Recall: 0.6214689265536724\n",
      "Accuracy: 0.6430868167202572\n",
      "[0.6185567  0.66465257]\n",
      "F1 Score (Macro): 0.6416046345033793\n"
     ]
    }
   ],
   "source": [
    "y_true=ground_truth_test\n",
    "y_pred=model_response_test\n",
    "\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "print(\"True Positives (TP):\", tp)\n",
    "print(\"False Positives (FP):\", fp)\n",
    "print(\"False Negatives (FN):\", fn)\n",
    "print(\"True Negatives (TN):\", tn)\n",
    "\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "f1_per_class = f1_score(y_true, y_pred, average=None)\n",
    "print(f1_per_class)\n",
    "\n",
    "print(\"F1 Score (Macro):\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c858a341",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     if macro_f1==best_dict['best_macro']:\n",
    "#         best_dict['all_best'].append(binary_vars)\n",
    "        \n",
    "        \n",
    "#         ground_truth_test, model_response_test = evaluate_model(\n",
    "#             data_reformated_test, test_responses, binary_vars, model_name, options, trial\n",
    "#         )\n",
    "\n",
    "\n",
    "#         macro_f1_test = f1_score(ground_truth_test, model_response_test, average='macro')\n",
    "#         best_dict['all_best_test'].append(macro_f1_test)\n",
    "        \n",
    "#         print('Got a new tie', best_dict['best_macro'],best_dict['best_test'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
